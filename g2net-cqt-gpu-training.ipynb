{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "western-childhood",
   "metadata": {
    "papermill": {
     "duration": 0.022305,
     "end_time": "2023-11-21T09:56:31.052597",
     "exception": false,
     "start_time": "2023-11-21T09:56:31.030292",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## About\n",
    "\n",
    "In this notebook, I will show a way to load data from TFRecord, and train a model implemented in PyTorch. In this way, you can get some advantages like\n",
    "\n",
    "1. You can speed up your training. TFRecord will allow you to load data faster than reading from npy file in your PyTorch Dataset and DataLoader.\n",
    "2. If you are using Google Colab (Pro/Pro+), you don't need to put your data on Google Drive. What you only need is the GCS path of the TFRecord dataset, which can be obtained through kaggle_datasets API.\n",
    "\n",
    "Of course, Tensorflow + TPU can speed up training even more, but some of you may want to use PyTorch maybe because it has `timm` or maybe because you are more used to use PyTorch that Tensorflow. This notebook is for those people.\n",
    "\n",
    "The whole pipeline is based on [Y.Nakama's notebook](https://www.kaggle.com/yasufuminakama/g2net-efficientnet-b7-baseline-training)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "secure-fitness",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-21T09:56:31.102312Z",
     "iopub.status.busy": "2023-11-21T09:56:31.101727Z",
     "iopub.status.idle": "2023-11-21T09:56:53.236452Z",
     "shell.execute_reply": "2023-11-21T09:56:53.235700Z",
     "shell.execute_reply.started": "2021-08-22T07:45:18.889689Z"
    },
    "papermill": {
     "duration": 22.164413,
     "end_time": "2023-11-21T09:56:53.236618",
     "exception": false,
     "start_time": "2023-11-21T09:56:31.072205",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install -q nnAudio\n",
    "!pip install -q ../input/pytorch-image-models/pytorch-image-models-master/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "floral-mixture",
   "metadata": {
    "papermill": {
     "duration": 0.020164,
     "end_time": "2023-11-21T09:56:53.277715",
     "exception": false,
     "start_time": "2023-11-21T09:56:53.257551",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "raised-television",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-21T09:56:53.324531Z",
     "iopub.status.busy": "2023-11-21T09:56:53.323893Z",
     "iopub.status.idle": "2023-11-21T09:57:01.534459Z",
     "shell.execute_reply": "2023-11-21T09:57:01.533532Z",
     "shell.execute_reply.started": "2021-08-22T08:10:09.927661Z"
    },
    "papermill": {
     "duration": 8.236666,
     "end_time": "2023-11-21T09:57:01.534613",
     "exception": false,
     "start_time": "2023-11-21T09:56:53.297947",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/nnAudio/Spectrogram.py:7: Warning: importing Spectrogram subpackage will be deprecated soon. You should import the feature extractor from the feature subpackage. See actual documentation.\n",
      "  category=Warning,\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "import math\n",
    "import random\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy as sp\n",
    "import tensorflow as tf  # for reading TFRecord Dataset\n",
    "import tensorflow_datasets as tfds  # for making tf.data.Dataset to return numpy arrays\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import timm\n",
    "from kaggle_datasets import KaggleDatasets\n",
    "from nnAudio.Spectrogram import CQT1992v2\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.model_selection import KFold\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "coordinate-pavilion",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-21T09:57:01.647204Z",
     "iopub.status.busy": "2023-11-21T09:57:01.646565Z",
     "iopub.status.idle": "2023-11-21T09:57:01.649870Z",
     "shell.execute_reply": "2023-11-21T09:57:01.649202Z",
     "shell.execute_reply.started": "2021-08-22T07:45:44.968748Z"
    },
    "papermill": {
     "duration": 0.095039,
     "end_time": "2023-11-21T09:57:01.650011",
     "exception": false,
     "start_time": "2023-11-21T09:57:01.554972",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "SAVEDIR = Path(\"./\")\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "stunning-christopher",
   "metadata": {
    "papermill": {
     "duration": 0.01976,
     "end_time": "2023-11-21T09:57:01.690379",
     "exception": false,
     "start_time": "2023-11-21T09:57:01.670619",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## CFG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "noble-click",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-21T09:57:01.736859Z",
     "iopub.status.busy": "2023-11-21T09:57:01.736238Z",
     "iopub.status.idle": "2023-11-21T09:57:01.738888Z",
     "shell.execute_reply": "2023-11-21T09:57:01.738397Z",
     "shell.execute_reply.started": "2021-08-22T07:45:47.258595Z"
    },
    "papermill": {
     "duration": 0.028952,
     "end_time": "2023-11-21T09:57:01.739011",
     "exception": false,
     "start_time": "2023-11-21T09:57:01.710059",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class CFG:\n",
    "    debug = False\n",
    "    print_freq = 50\n",
    "    num_workers = 4\n",
    "    model_name = \"tf_efficientnet_b0_ns\"\n",
    "    qtransform_params = {\"sr\": 2048, \"fmin\": 20, \"fmax\": 1024, \"hop_length\": 32, \"bins_per_octave\": 8}\n",
    "    scheduler = \"CosineAnnealingLR\"\n",
    "    epochs = 3\n",
    "    T_max = 3\n",
    "    lr = 1e-4\n",
    "    min_lr = 1e-7\n",
    "    batch_size = 64\n",
    "    weight_decay = 1e-3\n",
    "    gradient_accumulation_steps = 1\n",
    "    max_grad_norm = 1000\n",
    "    seed = 42\n",
    "    target_size = 1\n",
    "    target_col = \"target\"\n",
    "    n_fold = 5\n",
    "    trn_fold = [0]  # [0, 1, 2, 3, 4]\n",
    "    train = True\n",
    "\n",
    "if CFG.debug:\n",
    "    CFG.epochs = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "elect-latest",
   "metadata": {
    "papermill": {
     "duration": 0.01946,
     "end_time": "2023-11-21T09:57:01.778567",
     "exception": false,
     "start_time": "2023-11-21T09:57:01.759107",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "swedish-compiler",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-21T09:57:01.827219Z",
     "iopub.status.busy": "2023-11-21T09:57:01.826005Z",
     "iopub.status.idle": "2023-11-21T09:57:01.833195Z",
     "shell.execute_reply": "2023-11-21T09:57:01.832635Z",
     "shell.execute_reply.started": "2021-08-22T07:45:49.687879Z"
    },
    "papermill": {
     "duration": 0.034594,
     "end_time": "2023-11-21T09:57:01.833314",
     "exception": false,
     "start_time": "2023-11-21T09:57:01.798720",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ====================================================\n",
    "# Utils\n",
    "# ====================================================\n",
    "def get_score(y_true, y_pred):\n",
    "    score = roc_auc_score(y_true, y_pred)\n",
    "    return score\n",
    "\n",
    "\n",
    "def init_logger(log_file=SAVEDIR / 'train.log'):\n",
    "    from logging import getLogger, INFO, FileHandler,  Formatter,  StreamHandler\n",
    "    logger = getLogger(__name__)\n",
    "    logger.setLevel(INFO)\n",
    "    handler1 = StreamHandler()\n",
    "    handler1.setFormatter(Formatter(\"%(message)s\"))\n",
    "    handler2 = FileHandler(filename=log_file)\n",
    "    handler2.setFormatter(Formatter(\"%(message)s\"))\n",
    "    logger.addHandler(handler1)\n",
    "    logger.addHandler(handler2)\n",
    "    return logger\n",
    "\n",
    "LOGGER = init_logger()\n",
    "\n",
    "\n",
    "def seed_torch(seed=42):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "\n",
    "seed_torch(seed=CFG.seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ready-holder",
   "metadata": {
    "papermill": {
     "duration": 0.019557,
     "end_time": "2023-11-21T09:57:01.872899",
     "exception": false,
     "start_time": "2023-11-21T09:57:01.853342",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## TFRecord Loader\n",
    "\n",
    "This is the heart of this notebook. Instead of using PyTorch's Dataset and DataLoader, here I define custom Loader that reads samples from TFRecords.\n",
    "\n",
    "FYI, there's a library that does the same thing, but its implementation is not optimized, so it's slower.\n",
    "\n",
    "https://github.com/vahidk/tfrecord"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "enabling-capability",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-21T09:57:01.921785Z",
     "iopub.status.busy": "2023-11-21T09:57:01.921200Z",
     "iopub.status.idle": "2023-11-21T10:03:18.246833Z",
     "shell.execute_reply": "2023-11-21T10:03:18.246137Z",
     "shell.execute_reply.started": "2021-08-22T07:51:25.548449Z"
    },
    "papermill": {
     "duration": 376.354015,
     "end_time": "2023-11-21T10:03:18.246987",
     "exception": false,
     "start_time": "2023-11-21T09:57:01.892972",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gs://kds-a7cffbd8470ba9f7f2ef2becc81a3378eaba4bec88c1f790cd0123e4\n",
      "gs://kds-0ca7151be3b36ba2ead50cfd4a2a565b064563fe7795c9926a6b67a5\n",
      "gs://kds-94984261a36e134237c3a647edd6804fd5070fb97f325cc0aed3d850\n",
      "gs://kds-3a3d8959737f7e7af44f725317f74f51ba2618fc22a57bab5fb23ef6\n",
      "train_files:  20\n"
     ]
    }
   ],
   "source": [
    "gcs_paths = []\n",
    "for i, j in [(0, 4), (5, 9), (10, 14), (15, 19)]:\n",
    "    path = f\"g2net-waveform-tfrecords-train-{i}-{j}\"\n",
    "    n_trial = 0\n",
    "    while True:\n",
    "        try:\n",
    "            gcs_path = KaggleDatasets().get_gcs_path(path)\n",
    "            gcs_paths.append(gcs_path)\n",
    "            print(gcs_path)\n",
    "            break\n",
    "        except:\n",
    "            if n_trial > 10:\n",
    "                break\n",
    "            n_trial += 1\n",
    "            continue\n",
    "            \n",
    "all_files = []\n",
    "for path in gcs_paths:\n",
    "    all_files.extend(np.sort(np.array(tf.io.gfile.glob(path + \"/train*.tfrecords\"))))\n",
    "    \n",
    "print(\"train_files: \", len(all_files))\n",
    "all_files = np.array(all_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "western-compromise",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-21T10:03:18.295675Z",
     "iopub.status.busy": "2023-11-21T10:03:18.295025Z",
     "iopub.status.idle": "2023-11-21T10:03:18.298331Z",
     "shell.execute_reply": "2023-11-21T10:03:18.297737Z",
     "shell.execute_reply.started": "2021-08-22T07:53:24.992903Z"
    },
    "papermill": {
     "duration": 0.029291,
     "end_time": "2023-11-21T10:03:18.298451",
     "exception": false,
     "start_time": "2023-11-21T10:03:18.269160",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def count_data_items(fileids):\n",
    "    \"\"\"\n",
    "    Count the number of samples.\n",
    "    Each of the TFRecord datasets is designed to contain 28000 samples.\n",
    "    \"\"\"\n",
    "    return len(fileids) * 28000\n",
    "\n",
    "\n",
    "AUTO = tf.data.experimental.AUTOTUNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "empirical-component",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-21T10:03:18.358371Z",
     "iopub.status.busy": "2023-11-21T10:03:18.357757Z",
     "iopub.status.idle": "2023-11-21T10:03:18.361144Z",
     "shell.execute_reply": "2023-11-21T10:03:18.360542Z",
     "shell.execute_reply.started": "2021-08-22T08:11:32.070329Z"
    },
    "papermill": {
     "duration": 0.039963,
     "end_time": "2023-11-21T10:03:18.361293",
     "exception": false,
     "start_time": "2023-11-21T10:03:18.321330",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def prepare_wave(wave):\n",
    "    wave = tf.reshape(tf.io.decode_raw(wave, tf.float64), (3, 4096))\n",
    "    normalized_waves = []\n",
    "    for i in range(3):\n",
    "        normalized_wave = wave[i] / tf.math.reduce_max(wave[i])\n",
    "        normalized_waves.append(normalized_wave)\n",
    "    wave = tf.stack(normalized_waves, axis=0)\n",
    "    wave = tf.cast(wave, tf.float32)\n",
    "    return wave\n",
    "\n",
    "\n",
    "def read_labeled_tfrecord(example):\n",
    "    tfrec_format = {\n",
    "        \"wave\": tf.io.FixedLenFeature([], tf.string),\n",
    "        \"wave_id\": tf.io.FixedLenFeature([], tf.string),\n",
    "        \"target\": tf.io.FixedLenFeature([], tf.int64)\n",
    "    }\n",
    "    example = tf.io.parse_single_example(example, tfrec_format)\n",
    "    return prepare_wave(example[\"wave\"]), tf.reshape(tf.cast(example[\"target\"], tf.float32), [1]), example[\"wave_id\"]\n",
    "\n",
    "\n",
    "def read_unlabeled_tfrecord(example, return_image_id):\n",
    "    tfrec_format = {\n",
    "        \"wave\": tf.io.FixedLenFeature([], tf.string),\n",
    "        \"wave_id\": tf.io.FixedLenFeature([], tf.string)\n",
    "    }\n",
    "    example = tf.io.parse_single_example(example, tfrec_format)\n",
    "    return prepare_wave(example[\"wave\"]), example[\"wave_id\"] if return_image_id else 0\n",
    "\n",
    "\n",
    "def get_dataset(files, batch_size=16, repeat=False, cache=False, shuffle=False, labeled=True, return_image_ids=True):\n",
    "    ds = tf.data.TFRecordDataset(files, num_parallel_reads=AUTO, compression_type=\"GZIP\")\n",
    "    if cache:\n",
    "        # You'll need around 15GB RAM if you'd like to cache val dataset, and 50~60GB RAM for train dataset.\n",
    "        ds = ds.cache()\n",
    "\n",
    "    if repeat:\n",
    "        ds = ds.repeat()\n",
    "\n",
    "    if shuffle:\n",
    "        ds = ds.shuffle(1024 * 2)\n",
    "        opt = tf.data.Options()\n",
    "        opt.experimental_deterministic = False\n",
    "        ds = ds.with_options(opt)\n",
    "\n",
    "    if labeled:\n",
    "        ds = ds.map(read_labeled_tfrecord, num_parallel_calls=AUTO)\n",
    "    else:\n",
    "        ds = ds.map(lambda example: read_unlabeled_tfrecord(example, return_image_ids), num_parallel_calls=AUTO)\n",
    "\n",
    "    ds = ds.batch(batch_size)\n",
    "    ds = ds.prefetch(AUTO)\n",
    "    return tfds.as_numpy(ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "genetic-therapist",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-21T10:03:18.418678Z",
     "iopub.status.busy": "2023-11-21T10:03:18.417614Z",
     "iopub.status.idle": "2023-11-21T10:03:18.420655Z",
     "shell.execute_reply": "2023-11-21T10:03:18.420113Z",
     "shell.execute_reply.started": "2021-08-22T07:55:59.008992Z"
    },
    "papermill": {
     "duration": 0.036424,
     "end_time": "2023-11-21T10:03:18.420787",
     "exception": false,
     "start_time": "2023-11-21T10:03:18.384363",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class TFRecordDataLoader:\n",
    "    def __init__(self, files, batch_size=32, cache=False, train=True, repeat=False, shuffle=False, labeled=True, return_image_ids=True):\n",
    "        self.ds = get_dataset(\n",
    "            files, \n",
    "            batch_size=batch_size,\n",
    "            cache=cache,\n",
    "            repeat=repeat,\n",
    "            shuffle=shuffle,\n",
    "            labeled=labeled,\n",
    "            return_image_ids=return_image_ids)\n",
    "        \n",
    "        if train:\n",
    "            self.num_examples = count_data_items(files)\n",
    "        else:\n",
    "            self.num_examples = count_data_items_test(files)\n",
    "\n",
    "        self.batch_size = batch_size\n",
    "        self.labeled = labeled\n",
    "        self.return_image_ids = return_image_ids\n",
    "        self._iterator = None\n",
    "    \n",
    "    def __iter__(self):\n",
    "        if self._iterator is None:\n",
    "            self._iterator = iter(self.ds)\n",
    "        else:\n",
    "            self._reset()\n",
    "        return self._iterator\n",
    "\n",
    "    def _reset(self):\n",
    "        self._iterator = iter(self.ds)\n",
    "\n",
    "    def __next__(self):\n",
    "        batch = next(self._iterator)\n",
    "        return batch\n",
    "\n",
    "    def __len__(self):\n",
    "        n_batches = self.num_examples // self.batch_size\n",
    "        if self.num_examples % self.batch_size == 0:\n",
    "            return n_batches\n",
    "        else:\n",
    "            return n_batches + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "viral-pregnancy",
   "metadata": {
    "papermill": {
     "duration": 0.02252,
     "end_time": "2023-11-21T10:03:18.466833",
     "exception": false,
     "start_time": "2023-11-21T10:03:18.444313",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fresh-universe",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-21T10:03:18.519157Z",
     "iopub.status.busy": "2023-11-21T10:03:18.518523Z",
     "iopub.status.idle": "2023-11-21T10:03:18.521587Z",
     "shell.execute_reply": "2023-11-21T10:03:18.520986Z",
     "shell.execute_reply.started": "2021-08-22T07:57:46.799982Z"
    },
    "papermill": {
     "duration": 0.033097,
     "end_time": "2023-11-21T10:03:18.521718",
     "exception": false,
     "start_time": "2023-11-21T10:03:18.488621",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ====================================================\n",
    "# MODEL\n",
    "# ====================================================\n",
    "class CustomModel(nn.Module):\n",
    "    def __init__(self, cfg, pretrained=False):\n",
    "        super().__init__()\n",
    "        self.cfg = cfg\n",
    "        self.wave_transform = CQT1992v2(**CFG.qtransform_params)\n",
    "        self.model = timm.create_model(self.cfg.model_name, pretrained=pretrained, in_chans=3)\n",
    "        self.n_features = self.model.classifier.in_features\n",
    "        self.model.classifier = nn.Linear(self.n_features, self.cfg.target_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        waves = []\n",
    "        for i in range(3):\n",
    "            waves.append(self.wave_transform(x[:, i]))\n",
    "        x = torch.stack(waves, dim=1)\n",
    "        output = self.model(x)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "invalid-circus",
   "metadata": {
    "papermill": {
     "duration": 0.021881,
     "end_time": "2023-11-21T10:03:18.565052",
     "exception": false,
     "start_time": "2023-11-21T10:03:18.543171",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "operating-correspondence",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-21T10:03:18.619257Z",
     "iopub.status.busy": "2023-11-21T10:03:18.618310Z",
     "iopub.status.idle": "2023-11-21T10:03:18.621339Z",
     "shell.execute_reply": "2023-11-21T10:03:18.620751Z",
     "shell.execute_reply.started": "2021-08-22T07:58:27.742011Z"
    },
    "papermill": {
     "duration": 0.03434,
     "end_time": "2023-11-21T10:03:18.621466",
     "exception": false,
     "start_time": "2023-11-21T10:03:18.587126",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class AverageMeter(object):\n",
    "    \"\"\"Computes and stores the average and current value\"\"\"\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.val = 0\n",
    "        self.avg = 0\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "\n",
    "    def update(self, val, n=1):\n",
    "        self.val = val\n",
    "        self.sum += val * n\n",
    "        self.count += n\n",
    "        self.avg = self.sum / self.count\n",
    "\n",
    "\n",
    "def asMinutes(s):\n",
    "    m = math.floor(s / 60)\n",
    "    s -= m * 60\n",
    "    return '%dm %ds' % (m, s)\n",
    "\n",
    "\n",
    "def timeSince(since, percent):\n",
    "    now = time.time()\n",
    "    s = now - since\n",
    "    es = s / (percent)\n",
    "    rs = es - s\n",
    "    return '%s (remain %s)' % (asMinutes(s), asMinutes(rs))\n",
    "\n",
    "\n",
    "def max_memory_allocated():\n",
    "    MB = 1024.0 * 1024.0\n",
    "    mem = torch.cuda.max_memory_allocated() / MB\n",
    "    return f\"{mem:.0f} MB\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "written-dream",
   "metadata": {
    "papermill": {
     "duration": 0.021773,
     "end_time": "2023-11-21T10:03:18.664794",
     "exception": false,
     "start_time": "2023-11-21T10:03:18.643021",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "expanded-shower",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-21T10:03:18.731689Z",
     "iopub.status.busy": "2023-11-21T10:03:18.730925Z",
     "iopub.status.idle": "2023-11-21T10:03:18.733842Z",
     "shell.execute_reply": "2023-11-21T10:03:18.733196Z",
     "shell.execute_reply.started": "2021-08-22T08:43:00.41243Z"
    },
    "papermill": {
     "duration": 0.047291,
     "end_time": "2023-11-21T10:03:18.733983",
     "exception": false,
     "start_time": "2023-11-21T10:03:18.686692",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def train_fn(files, model, criterion, optimizer, epoch, scheduler, device):\n",
    "    batch_time = AverageMeter()\n",
    "    data_time = AverageMeter()\n",
    "    losses = AverageMeter()\n",
    "    scores = AverageMeter()\n",
    "\n",
    "    # switch to train mode\n",
    "    model.train()\n",
    "    start = end = time.time()\n",
    "    global_step = 0\n",
    "\n",
    "    train_loader = TFRecordDataLoader(\n",
    "        files, batch_size=CFG.batch_size, shuffle=True)\n",
    "    for step, d in enumerate(train_loader):\n",
    "        # measure data loading time\n",
    "        data_time.update(time.time() - end)\n",
    "        images = torch.from_numpy(d[0]).to(device)\n",
    "        labels = torch.from_numpy(d[1]).to(device)\n",
    "\n",
    "        batch_size = labels.size(0)\n",
    "        y_preds = model(images)\n",
    "        loss = criterion(y_preds.view(-1), labels.view(-1))\n",
    "        # record loss\n",
    "        losses.update(loss.item(), batch_size)\n",
    "        if CFG.gradient_accumulation_steps > 1:\n",
    "            loss = loss / CFG.gradient_accumulation_steps\n",
    "        loss.backward()\n",
    "        grad_norm = torch.nn.utils.clip_grad_norm_(model.parameters(), CFG.max_grad_norm)\n",
    "        if (step + 1) % CFG.gradient_accumulation_steps == 0:\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "            global_step += 1\n",
    "        # measure elapsed time\n",
    "        batch_time.update(time.time() - end)\n",
    "        end = time.time()\n",
    "        if step % CFG.print_freq == 0:\n",
    "            print('Epoch: [{0}/{1}][{2}/{3}] '\n",
    "                  'Loss: {loss.val:.4f}({loss.avg:.4f}) '\n",
    "                  'Grad: {grad_norm:.4f}  '\n",
    "                  'LR: {lr:.6f}  '\n",
    "                  'Elapsed: {remain:s} '\n",
    "                  'Max mem: {mem:s}'\n",
    "                  .format(\n",
    "                   epoch+1, CFG.epochs, step, len(train_loader),\n",
    "                   loss=losses,\n",
    "                   grad_norm=grad_norm,\n",
    "                   lr=scheduler.get_lr()[0],\n",
    "                   remain=timeSince(start, float(step + 1) / len(train_loader)),\n",
    "                   mem=max_memory_allocated()))\n",
    "    return losses.avg\n",
    "\n",
    "\n",
    "def valid_fn(files, model, criterion, device):\n",
    "    batch_time = AverageMeter()\n",
    "    data_time = AverageMeter()\n",
    "    losses = AverageMeter()\n",
    "    scores = AverageMeter()\n",
    "    # switch to evaluation mode\n",
    "    model.eval()\n",
    "    filenames = []\n",
    "    targets = []\n",
    "    preds = []\n",
    "    start = end = time.time()\n",
    "    valid_loader = TFRecordDataLoader(\n",
    "        files, batch_size=CFG.batch_size * 2, shuffle=False)\n",
    "    for step, d in enumerate(valid_loader):\n",
    "        # measure data loading time\n",
    "        data_time.update(time.time() - end)\n",
    "        \n",
    "        targets.extend(d[1].reshape(-1).tolist())\n",
    "        filenames.extend([f.decode(\"UTF-8\") for f in d[2]])\n",
    "        \n",
    "        images = torch.from_numpy(d[0]).to(device)\n",
    "        labels = torch.from_numpy(d[1]).to(device)\n",
    "\n",
    "        batch_size = labels.size(0)\n",
    "        # compute loss\n",
    "        with torch.no_grad():\n",
    "            y_preds = model(images)\n",
    "        loss = criterion(y_preds.view(-1), labels.view(-1))\n",
    "        losses.update(loss.item(), batch_size)\n",
    "\n",
    "        preds.append(y_preds.sigmoid().to('cpu').numpy())\n",
    "        if CFG.gradient_accumulation_steps > 1:\n",
    "            loss = loss / CFG.gradient_accumulation_steps\n",
    "        # measure elapsed time\n",
    "        batch_time.update(time.time() - end)\n",
    "        end = time.time()\n",
    "        if step % CFG.print_freq == 0:\n",
    "            print('EVAL: [{0}/{1}] '\n",
    "                  'Data {data_time.val:.3f} ({data_time.avg:.3f}) '\n",
    "                  'Elapsed {remain:s} '\n",
    "                  'Loss: {loss.val:.4f}({loss.avg:.4f}) '\n",
    "                  .format(\n",
    "                   step, len(valid_loader), batch_time=batch_time,\n",
    "                   data_time=data_time, loss=losses,\n",
    "                   remain=timeSince(start, float(step+1)/len(valid_loader)),\n",
    "                   ))\n",
    "    predictions = np.concatenate(preds).reshape(-1)\n",
    "    return losses.avg, predictions, np.array(targets), np.array(filenames)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "distinct-better",
   "metadata": {
    "papermill": {
     "duration": 0.021126,
     "end_time": "2023-11-21T10:03:18.776965",
     "exception": false,
     "start_time": "2023-11-21T10:03:18.755839",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Train loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "rocky-paint",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2023-11-21T10:03:18.837941Z",
     "iopub.status.busy": "2023-11-21T10:03:18.836973Z",
     "iopub.status.idle": "2023-11-21T10:03:18.839776Z",
     "shell.execute_reply": "2023-11-21T10:03:18.839209Z",
     "shell.execute_reply.started": "2021-08-22T09:23:07.020705Z"
    },
    "papermill": {
     "duration": 0.04115,
     "end_time": "2023-11-21T10:03:18.839919",
     "exception": false,
     "start_time": "2023-11-21T10:03:18.798769",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ====================================================\n",
    "# Train loop\n",
    "# ====================================================\n",
    "def train_loop(train_tfrecords: np.ndarray, val_tfrecords: np.ndarray, fold: int):\n",
    "    \n",
    "    LOGGER.info(f\"========== fold: {fold} training ==========\")\n",
    "    \n",
    "    # ====================================================\n",
    "    # scheduler \n",
    "    # ====================================================\n",
    "    def get_scheduler(optimizer):\n",
    "        if CFG.scheduler=='ReduceLROnPlateau':\n",
    "            scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=CFG.factor, patience=CFG.patience, verbose=True, eps=CFG.eps)\n",
    "        elif CFG.scheduler=='CosineAnnealingLR':\n",
    "            scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=CFG.T_max, eta_min=CFG.min_lr, last_epoch=-1)\n",
    "        elif CFG.scheduler=='CosineAnnealingWarmRestarts':\n",
    "            scheduler = optim.lr_scheduler.CosineAnnealingWarmRestarts(optimizer, T_0=CFG.T_0, T_mult=1, eta_min=CFG.min_lr, last_epoch=-1)\n",
    "        return scheduler\n",
    "\n",
    "    # ====================================================\n",
    "    # model & optimizer\n",
    "    # ====================================================\n",
    "    model = CustomModel(CFG, pretrained=True)\n",
    "    model.to(device)\n",
    "\n",
    "    optimizer = optim.Adam(model.parameters(), lr=CFG.lr, weight_decay=CFG.weight_decay, amsgrad=False)\n",
    "    scheduler = get_scheduler(optimizer)\n",
    "\n",
    "    # ====================================================\n",
    "    # loop\n",
    "    # ====================================================\n",
    "    criterion = nn.BCEWithLogitsLoss()\n",
    "\n",
    "    best_score = 0.\n",
    "    best_loss = np.inf\n",
    "    \n",
    "    for epoch in range(CFG.epochs):\n",
    "        \n",
    "        start_time = time.time()\n",
    "        \n",
    "        # train\n",
    "        avg_loss = train_fn(train_tfrecords, model, criterion, optimizer, epoch, scheduler, device)\n",
    "\n",
    "        # eval\n",
    "        avg_val_loss, preds, targets, files = valid_fn(val_tfrecords, model, criterion, device)\n",
    "        valid_result_df = pd.DataFrame({\"target\": targets, \"preds\": preds, \"id\": files})\n",
    "        \n",
    "        if isinstance(scheduler, optim.lr_scheduler.ReduceLROnPlateau):\n",
    "            scheduler.step(avg_val_loss)\n",
    "        elif isinstance(scheduler, optim.lr_scheduler.CosineAnnealingLR):\n",
    "            scheduler.step()\n",
    "        elif isinstance(scheduler, optim.lr_scheduler.CosineAnnealingWarmRestarts):\n",
    "            scheduler.step()\n",
    "\n",
    "        # scoring\n",
    "        score = get_score(targets, preds)\n",
    "\n",
    "        elapsed = time.time() - start_time\n",
    "\n",
    "        LOGGER.info(f'Epoch {epoch+1} - avg_train_loss: {avg_loss:.4f}  avg_val_loss: {avg_val_loss:.4f}  time: {elapsed:.0f}s')\n",
    "        LOGGER.info(f'Epoch {epoch+1} - Score: {score:.4f}')\n",
    "\n",
    "        if score > best_score:\n",
    "            best_score = score\n",
    "            LOGGER.info(f'Epoch {epoch+1} - Save Best Score: {best_score:.4f} Model')\n",
    "            torch.save({'model': model.state_dict(), \n",
    "                        'preds': preds},\n",
    "                        SAVEDIR / f'{CFG.model_name}_fold{fold}_best_score.pth')\n",
    "        \n",
    "        if avg_val_loss < best_loss:\n",
    "            best_loss = avg_val_loss\n",
    "            LOGGER.info(f'Epoch {epoch+1} - Save Best Loss: {best_loss:.4f} Model')\n",
    "            torch.save({'model': model.state_dict(), \n",
    "                        'preds': preds},\n",
    "                        SAVEDIR / f'{CFG.model_name}_fold{fold}_best_loss.pth')\n",
    "    \n",
    "    valid_result_df[\"preds\"] = torch.load(SAVEDIR / f\"{CFG.model_name}_fold{fold}_best_loss.pth\",\n",
    "                                          map_location=\"cpu\")[\"preds\"]\n",
    "\n",
    "    return valid_result_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "joint-envelope",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-21T10:03:18.894156Z",
     "iopub.status.busy": "2023-11-21T10:03:18.893436Z",
     "iopub.status.idle": "2023-11-21T10:03:18.896346Z",
     "shell.execute_reply": "2023-11-21T10:03:18.895818Z",
     "shell.execute_reply.started": "2021-08-22T09:23:07.971832Z"
    },
    "papermill": {
     "duration": 0.033567,
     "end_time": "2023-11-21T10:03:18.896480",
     "exception": false,
     "start_time": "2023-11-21T10:03:18.862913",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def main():\n",
    "    def get_result(result_df):\n",
    "        preds = result_df['preds'].values\n",
    "        labels = result_df[CFG.target_col].values\n",
    "        score = get_score(labels, preds)\n",
    "        LOGGER.info(f'Score: {score:<.4f}')\n",
    "    \n",
    "    if CFG.train:\n",
    "        # train \n",
    "        oof_df = pd.DataFrame()\n",
    "        kf = KFold(n_splits=CFG.n_fold, shuffle=True, random_state=CFG.seed)\n",
    "\n",
    "        folds = list(kf.split(all_files))\n",
    "        for fold in range(CFG.n_fold):\n",
    "            if fold in CFG.trn_fold:\n",
    "                trn_idx, val_idx = folds[fold]\n",
    "                train_files = all_files[trn_idx]\n",
    "                valid_files = all_files[val_idx]\n",
    "                _oof_df = train_loop(train_files, valid_files, fold)\n",
    "                oof_df = pd.concat([oof_df, _oof_df])\n",
    "                LOGGER.info(f\"========== fold: {fold} result ==========\")\n",
    "                get_result(_oof_df)\n",
    "        # CV result\n",
    "        LOGGER.info(f\"========== CV ==========\")\n",
    "        get_result(oof_df)\n",
    "        # save result\n",
    "        oof_df.to_csv(SAVEDIR / 'oof_df.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "amber-national",
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a",
    "execution": {
     "iopub.execute_input": "2023-11-21T10:03:18.947500Z",
     "iopub.status.busy": "2023-11-21T10:03:18.946348Z",
     "iopub.status.idle": "2023-11-21T11:17:08.086125Z",
     "shell.execute_reply": "2023-11-21T11:17:08.086599Z"
    },
    "papermill": {
     "duration": 4429.168312,
     "end_time": "2023-11-21T11:17:08.086810",
     "exception": false,
     "start_time": "2023-11-21T10:03:18.918498",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "========== fold: 0 training ==========\n",
      "/opt/conda/lib/python3.7/site-packages/nnAudio/utils.py:429: SyntaxWarning: If fmax is given, n_bins will be ignored\n",
      "  warnings.warn(\"If fmax is given, n_bins will be ignored\", SyntaxWarning)\n",
      "Downloading: \"https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-weights/tf_efficientnet_b0_ns-c0e6a31c.pth\" to /root/.cache/torch/hub/checkpoints/tf_efficientnet_b0_ns-c0e6a31c.pth\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CQT kernels created, time used = 0.0333 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/torch/optim/lr_scheduler.py:509: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  \"please use `get_last_lr()`.\", UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [1/3][0/7000] Loss: 0.6844(0.6844) Grad: 1.0282  LR: 0.000100  Elapsed: 0m 5s (remain 636m 44s) Max mem: 790 MB\n",
      "Epoch: [1/3][50/7000] Loss: 0.6773(0.6922) Grad: 0.9957  LR: 0.000100  Elapsed: 0m 14s (remain 33m 31s) Max mem: 834 MB\n",
      "Epoch: [1/3][100/7000] Loss: 0.6599(0.6878) Grad: 1.0817  LR: 0.000100  Elapsed: 0m 23s (remain 27m 13s) Max mem: 834 MB\n",
      "Epoch: [1/3][150/7000] Loss: 0.6391(0.6736) Grad: 1.3781  LR: 0.000100  Elapsed: 0m 33s (remain 25m 20s) Max mem: 834 MB\n",
      "Epoch: [1/3][200/7000] Loss: 0.6055(0.6568) Grad: 1.3227  LR: 0.000100  Elapsed: 0m 42s (remain 24m 12s) Max mem: 834 MB\n",
      "Epoch: [1/3][250/7000] Loss: 0.5627(0.6410) Grad: 1.0996  LR: 0.000100  Elapsed: 0m 52s (remain 23m 22s) Max mem: 834 MB\n",
      "Epoch: [1/3][300/7000] Loss: 0.5827(0.6266) Grad: 1.3715  LR: 0.000100  Elapsed: 1m 1s (remain 22m 49s) Max mem: 834 MB\n",
      "Epoch: [1/3][350/7000] Loss: 0.6221(0.6173) Grad: 1.3738  LR: 0.000100  Elapsed: 1m 11s (remain 22m 28s) Max mem: 834 MB\n",
      "Epoch: [1/3][400/7000] Loss: 0.5082(0.6086) Grad: 1.1638  LR: 0.000100  Elapsed: 1m 20s (remain 22m 2s) Max mem: 834 MB\n",
      "Epoch: [1/3][450/7000] Loss: 0.5826(0.5992) Grad: 1.5081  LR: 0.000100  Elapsed: 1m 29s (remain 21m 42s) Max mem: 834 MB\n",
      "Epoch: [1/3][500/7000] Loss: 0.5708(0.5937) Grad: 1.1700  LR: 0.000100  Elapsed: 1m 39s (remain 21m 25s) Max mem: 834 MB\n",
      "Epoch: [1/3][550/7000] Loss: 0.5585(0.5880) Grad: 1.1657  LR: 0.000100  Elapsed: 1m 48s (remain 21m 6s) Max mem: 834 MB\n",
      "Epoch: [1/3][600/7000] Loss: 0.5810(0.5825) Grad: 1.1136  LR: 0.000100  Elapsed: 1m 57s (remain 20m 51s) Max mem: 834 MB\n",
      "Epoch: [1/3][650/7000] Loss: 0.4225(0.5768) Grad: 0.7679  LR: 0.000100  Elapsed: 2m 7s (remain 20m 40s) Max mem: 834 MB\n",
      "Epoch: [1/3][700/7000] Loss: 0.4502(0.5723) Grad: 0.8048  LR: 0.000100  Elapsed: 2m 16s (remain 20m 24s) Max mem: 834 MB\n",
      "Epoch: [1/3][750/7000] Loss: 0.5059(0.5682) Grad: 0.7966  LR: 0.000100  Elapsed: 2m 25s (remain 20m 11s) Max mem: 834 MB\n",
      "Epoch: [1/3][800/7000] Loss: 0.4659(0.5644) Grad: 0.9061  LR: 0.000100  Elapsed: 2m 35s (remain 20m 0s) Max mem: 834 MB\n",
      "Epoch: [1/3][850/7000] Loss: 0.5824(0.5607) Grad: 1.0810  LR: 0.000100  Elapsed: 2m 44s (remain 19m 46s) Max mem: 834 MB\n",
      "Epoch: [1/3][900/7000] Loss: 0.4876(0.5582) Grad: 0.6706  LR: 0.000100  Elapsed: 2m 53s (remain 19m 34s) Max mem: 834 MB\n",
      "Epoch: [1/3][950/7000] Loss: 0.4735(0.5551) Grad: 0.8340  LR: 0.000100  Elapsed: 3m 2s (remain 19m 22s) Max mem: 834 MB\n",
      "Epoch: [1/3][1000/7000] Loss: 0.4784(0.5522) Grad: 0.8408  LR: 0.000100  Elapsed: 3m 12s (remain 19m 11s) Max mem: 834 MB\n",
      "Epoch: [1/3][1050/7000] Loss: 0.4747(0.5492) Grad: 0.8039  LR: 0.000100  Elapsed: 3m 21s (remain 18m 59s) Max mem: 834 MB\n",
      "Epoch: [1/3][1100/7000] Loss: 0.4501(0.5468) Grad: 0.9797  LR: 0.000100  Elapsed: 3m 30s (remain 18m 48s) Max mem: 834 MB\n",
      "Epoch: [1/3][1150/7000] Loss: 0.5567(0.5445) Grad: 1.0524  LR: 0.000100  Elapsed: 3m 39s (remain 18m 36s) Max mem: 834 MB\n",
      "Epoch: [1/3][1200/7000] Loss: 0.3954(0.5418) Grad: 0.7463  LR: 0.000100  Elapsed: 3m 48s (remain 18m 25s) Max mem: 834 MB\n",
      "Epoch: [1/3][1250/7000] Loss: 0.4473(0.5392) Grad: 0.7877  LR: 0.000100  Elapsed: 3m 58s (remain 18m 13s) Max mem: 834 MB\n",
      "Epoch: [1/3][1300/7000] Loss: 0.5220(0.5369) Grad: 0.9418  LR: 0.000100  Elapsed: 4m 7s (remain 18m 3s) Max mem: 834 MB\n",
      "Epoch: [1/3][1350/7000] Loss: 0.5343(0.5350) Grad: 0.9194  LR: 0.000100  Elapsed: 4m 16s (remain 17m 52s) Max mem: 834 MB\n",
      "Epoch: [1/3][1400/7000] Loss: 0.4675(0.5338) Grad: 0.5292  LR: 0.000100  Elapsed: 4m 25s (remain 17m 41s) Max mem: 834 MB\n",
      "Epoch: [1/3][1450/7000] Loss: 0.4975(0.5323) Grad: 0.7844  LR: 0.000100  Elapsed: 4m 35s (remain 17m 31s) Max mem: 834 MB\n",
      "Epoch: [1/3][1500/7000] Loss: 0.4128(0.5307) Grad: 0.8227  LR: 0.000100  Elapsed: 4m 44s (remain 17m 21s) Max mem: 834 MB\n",
      "Epoch: [1/3][1550/7000] Loss: 0.4320(0.5294) Grad: 0.6961  LR: 0.000100  Elapsed: 4m 53s (remain 17m 10s) Max mem: 834 MB\n",
      "Epoch: [1/3][1600/7000] Loss: 0.4245(0.5279) Grad: 0.9759  LR: 0.000100  Elapsed: 5m 2s (remain 17m 0s) Max mem: 834 MB\n",
      "Epoch: [1/3][1650/7000] Loss: 0.4641(0.5265) Grad: 0.6020  LR: 0.000100  Elapsed: 5m 11s (remain 16m 50s) Max mem: 834 MB\n",
      "Epoch: [1/3][1700/7000] Loss: 0.4741(0.5254) Grad: 0.7628  LR: 0.000100  Elapsed: 5m 21s (remain 16m 40s) Max mem: 834 MB\n",
      "Epoch: [1/3][1750/7000] Loss: 0.3951(0.5242) Grad: 0.7800  LR: 0.000100  Elapsed: 5m 30s (remain 16m 30s) Max mem: 834 MB\n",
      "Epoch: [1/3][1800/7000] Loss: 0.4149(0.5227) Grad: 0.9799  LR: 0.000100  Elapsed: 5m 39s (remain 16m 21s) Max mem: 834 MB\n",
      "Epoch: [1/3][1850/7000] Loss: 0.3784(0.5215) Grad: 0.6892  LR: 0.000100  Elapsed: 5m 49s (remain 16m 11s) Max mem: 834 MB\n",
      "Epoch: [1/3][1900/7000] Loss: 0.5218(0.5205) Grad: 0.9996  LR: 0.000100  Elapsed: 5m 58s (remain 16m 1s) Max mem: 834 MB\n",
      "Epoch: [1/3][1950/7000] Loss: 0.4855(0.5189) Grad: 1.1213  LR: 0.000100  Elapsed: 6m 7s (remain 15m 52s) Max mem: 834 MB\n",
      "Epoch: [1/3][2000/7000] Loss: 0.4830(0.5179) Grad: 0.6379  LR: 0.000100  Elapsed: 6m 17s (remain 15m 41s) Max mem: 834 MB\n",
      "Epoch: [1/3][2050/7000] Loss: 0.5581(0.5166) Grad: 0.8368  LR: 0.000100  Elapsed: 6m 26s (remain 15m 31s) Max mem: 834 MB\n",
      "Epoch: [1/3][2100/7000] Loss: 0.4943(0.5153) Grad: 0.6639  LR: 0.000100  Elapsed: 6m 35s (remain 15m 21s) Max mem: 834 MB\n",
      "Epoch: [1/3][2150/7000] Loss: 0.4359(0.5144) Grad: 0.7662  LR: 0.000100  Elapsed: 6m 44s (remain 15m 12s) Max mem: 834 MB\n",
      "Epoch: [1/3][2200/7000] Loss: 0.4707(0.5135) Grad: 0.7752  LR: 0.000100  Elapsed: 6m 53s (remain 15m 2s) Max mem: 834 MB\n",
      "Epoch: [1/3][2250/7000] Loss: 0.4424(0.5125) Grad: 0.7166  LR: 0.000100  Elapsed: 7m 3s (remain 14m 52s) Max mem: 834 MB\n",
      "Epoch: [1/3][2300/7000] Loss: 0.4573(0.5117) Grad: 0.6597  LR: 0.000100  Elapsed: 7m 12s (remain 14m 44s) Max mem: 834 MB\n",
      "Epoch: [1/3][2350/7000] Loss: 0.4134(0.5109) Grad: 0.6113  LR: 0.000100  Elapsed: 7m 22s (remain 14m 34s) Max mem: 834 MB\n",
      "Epoch: [1/3][2400/7000] Loss: 0.5127(0.5099) Grad: 0.8124  LR: 0.000100  Elapsed: 7m 31s (remain 14m 24s) Max mem: 834 MB\n",
      "Epoch: [1/3][2450/7000] Loss: 0.3760(0.5091) Grad: 0.8749  LR: 0.000100  Elapsed: 7m 40s (remain 14m 14s) Max mem: 834 MB\n",
      "Epoch: [1/3][2500/7000] Loss: 0.3659(0.5081) Grad: 0.6553  LR: 0.000100  Elapsed: 7m 49s (remain 14m 5s) Max mem: 834 MB\n",
      "Epoch: [1/3][2550/7000] Loss: 0.5214(0.5073) Grad: 1.0347  LR: 0.000100  Elapsed: 7m 59s (remain 13m 55s) Max mem: 834 MB\n",
      "Epoch: [1/3][2600/7000] Loss: 0.3600(0.5065) Grad: 0.6541  LR: 0.000100  Elapsed: 8m 8s (remain 13m 46s) Max mem: 834 MB\n",
      "Epoch: [1/3][2650/7000] Loss: 0.4766(0.5057) Grad: 1.0072  LR: 0.000100  Elapsed: 8m 18s (remain 13m 37s) Max mem: 834 MB\n",
      "Epoch: [1/3][2700/7000] Loss: 0.4701(0.5052) Grad: 0.7567  LR: 0.000100  Elapsed: 8m 27s (remain 13m 27s) Max mem: 834 MB\n",
      "Epoch: [1/3][2750/7000] Loss: 0.4006(0.5043) Grad: 0.6071  LR: 0.000100  Elapsed: 8m 36s (remain 13m 18s) Max mem: 834 MB\n",
      "Epoch: [1/3][2800/7000] Loss: 0.4618(0.5037) Grad: 0.8647  LR: 0.000100  Elapsed: 8m 46s (remain 13m 8s) Max mem: 834 MB\n",
      "Epoch: [1/3][2850/7000] Loss: 0.4343(0.5030) Grad: 0.5750  LR: 0.000100  Elapsed: 8m 55s (remain 12m 59s) Max mem: 834 MB\n",
      "Epoch: [1/3][2900/7000] Loss: 0.4431(0.5025) Grad: 0.9020  LR: 0.000100  Elapsed: 9m 4s (remain 12m 49s) Max mem: 834 MB\n",
      "Epoch: [1/3][2950/7000] Loss: 0.3625(0.5019) Grad: 0.6184  LR: 0.000100  Elapsed: 9m 14s (remain 12m 40s) Max mem: 834 MB\n",
      "Epoch: [1/3][3000/7000] Loss: 0.4420(0.5015) Grad: 0.6422  LR: 0.000100  Elapsed: 9m 23s (remain 12m 30s) Max mem: 834 MB\n",
      "Epoch: [1/3][3050/7000] Loss: 0.4417(0.5010) Grad: 0.6208  LR: 0.000100  Elapsed: 9m 32s (remain 12m 21s) Max mem: 834 MB\n",
      "Epoch: [1/3][3100/7000] Loss: 0.3935(0.5005) Grad: 0.7047  LR: 0.000100  Elapsed: 9m 41s (remain 12m 11s) Max mem: 834 MB\n",
      "Epoch: [1/3][3150/7000] Loss: 0.3978(0.4998) Grad: 0.6309  LR: 0.000100  Elapsed: 9m 51s (remain 12m 2s) Max mem: 834 MB\n",
      "Epoch: [1/3][3200/7000] Loss: 0.4777(0.4992) Grad: 0.8573  LR: 0.000100  Elapsed: 10m 0s (remain 11m 52s) Max mem: 834 MB\n",
      "Epoch: [1/3][3250/7000] Loss: 0.3797(0.4985) Grad: 0.6285  LR: 0.000100  Elapsed: 10m 9s (remain 11m 43s) Max mem: 834 MB\n",
      "Epoch: [1/3][3300/7000] Loss: 0.4447(0.4982) Grad: 0.7981  LR: 0.000100  Elapsed: 10m 19s (remain 11m 33s) Max mem: 834 MB\n",
      "Epoch: [1/3][3350/7000] Loss: 0.4357(0.4976) Grad: 0.7550  LR: 0.000100  Elapsed: 10m 28s (remain 11m 24s) Max mem: 834 MB\n",
      "Epoch: [1/3][3400/7000] Loss: 0.5032(0.4972) Grad: 1.0754  LR: 0.000100  Elapsed: 10m 37s (remain 11m 14s) Max mem: 834 MB\n",
      "Epoch: [1/3][3450/7000] Loss: 0.6015(0.4964) Grad: 1.0232  LR: 0.000100  Elapsed: 10m 46s (remain 11m 4s) Max mem: 834 MB\n",
      "Epoch: [1/3][3500/7000] Loss: 0.5516(0.4963) Grad: 0.7595  LR: 0.000100  Elapsed: 10m 55s (remain 10m 55s) Max mem: 834 MB\n",
      "Epoch: [1/3][3550/7000] Loss: 0.4107(0.4956) Grad: 0.6078  LR: 0.000100  Elapsed: 11m 5s (remain 10m 46s) Max mem: 834 MB\n",
      "Epoch: [1/3][3600/7000] Loss: 0.4799(0.4950) Grad: 0.8135  LR: 0.000100  Elapsed: 11m 14s (remain 10m 36s) Max mem: 834 MB\n",
      "Epoch: [1/3][3650/7000] Loss: 0.4989(0.4944) Grad: 0.8336  LR: 0.000100  Elapsed: 11m 23s (remain 10m 27s) Max mem: 834 MB\n",
      "Epoch: [1/3][3700/7000] Loss: 0.4644(0.4939) Grad: 0.8662  LR: 0.000100  Elapsed: 11m 32s (remain 10m 17s) Max mem: 834 MB\n",
      "Epoch: [1/3][3750/7000] Loss: 0.4893(0.4933) Grad: 0.6340  LR: 0.000100  Elapsed: 11m 42s (remain 10m 8s) Max mem: 834 MB\n",
      "Epoch: [1/3][3800/7000] Loss: 0.4071(0.4930) Grad: 0.6367  LR: 0.000100  Elapsed: 11m 51s (remain 9m 58s) Max mem: 834 MB\n",
      "Epoch: [1/3][3850/7000] Loss: 0.4314(0.4924) Grad: 0.7186  LR: 0.000100  Elapsed: 12m 0s (remain 9m 49s) Max mem: 834 MB\n",
      "Epoch: [1/3][3900/7000] Loss: 0.4303(0.4919) Grad: 0.7307  LR: 0.000100  Elapsed: 12m 9s (remain 9m 39s) Max mem: 834 MB\n",
      "Epoch: [1/3][3950/7000] Loss: 0.4242(0.4914) Grad: 0.5976  LR: 0.000100  Elapsed: 12m 19s (remain 9m 30s) Max mem: 834 MB\n",
      "Epoch: [1/3][4000/7000] Loss: 0.4580(0.4910) Grad: 1.0100  LR: 0.000100  Elapsed: 12m 28s (remain 9m 20s) Max mem: 834 MB\n",
      "Epoch: [1/3][4050/7000] Loss: 0.4334(0.4903) Grad: 0.5879  LR: 0.000100  Elapsed: 12m 37s (remain 9m 11s) Max mem: 834 MB\n",
      "Epoch: [1/3][4100/7000] Loss: 0.3651(0.4900) Grad: 0.6112  LR: 0.000100  Elapsed: 12m 46s (remain 9m 1s) Max mem: 834 MB\n",
      "Epoch: [1/3][4150/7000] Loss: 0.4250(0.4895) Grad: 1.1159  LR: 0.000100  Elapsed: 12m 56s (remain 8m 52s) Max mem: 834 MB\n",
      "Epoch: [1/3][4200/7000] Loss: 0.4693(0.4891) Grad: 0.8863  LR: 0.000100  Elapsed: 13m 5s (remain 8m 43s) Max mem: 834 MB\n",
      "Epoch: [1/3][4250/7000] Loss: 0.5197(0.4890) Grad: 0.8103  LR: 0.000100  Elapsed: 13m 14s (remain 8m 33s) Max mem: 834 MB\n",
      "Epoch: [1/3][4300/7000] Loss: 0.4054(0.4886) Grad: 0.6617  LR: 0.000100  Elapsed: 13m 23s (remain 8m 24s) Max mem: 834 MB\n",
      "Epoch: [1/3][4350/7000] Loss: 0.4474(0.4884) Grad: 0.5181  LR: 0.000100  Elapsed: 13m 32s (remain 8m 14s) Max mem: 834 MB\n",
      "Epoch: [1/3][4400/7000] Loss: 0.4611(0.4881) Grad: 0.5488  LR: 0.000100  Elapsed: 13m 42s (remain 8m 5s) Max mem: 834 MB\n",
      "Epoch: [1/3][4450/7000] Loss: 0.4948(0.4877) Grad: 0.5999  LR: 0.000100  Elapsed: 13m 51s (remain 7m 56s) Max mem: 834 MB\n",
      "Epoch: [1/3][4500/7000] Loss: 0.5922(0.4874) Grad: 1.5543  LR: 0.000100  Elapsed: 14m 0s (remain 7m 46s) Max mem: 834 MB\n",
      "Epoch: [1/3][4550/7000] Loss: 0.4689(0.4871) Grad: 0.9042  LR: 0.000100  Elapsed: 14m 10s (remain 7m 37s) Max mem: 834 MB\n",
      "Epoch: [1/3][4600/7000] Loss: 0.5259(0.4869) Grad: 0.6626  LR: 0.000100  Elapsed: 14m 19s (remain 7m 28s) Max mem: 834 MB\n",
      "Epoch: [1/3][4650/7000] Loss: 0.4707(0.4867) Grad: 0.5656  LR: 0.000100  Elapsed: 14m 29s (remain 7m 18s) Max mem: 834 MB\n",
      "Epoch: [1/3][4700/7000] Loss: 0.5336(0.4864) Grad: 1.0282  LR: 0.000100  Elapsed: 14m 38s (remain 7m 9s) Max mem: 834 MB\n",
      "Epoch: [1/3][4750/7000] Loss: 0.4514(0.4862) Grad: 0.5846  LR: 0.000100  Elapsed: 14m 47s (remain 7m 0s) Max mem: 834 MB\n",
      "Epoch: [1/3][4800/7000] Loss: 0.4777(0.4860) Grad: 0.5915  LR: 0.000100  Elapsed: 14m 57s (remain 6m 50s) Max mem: 834 MB\n",
      "Epoch: [1/3][4850/7000] Loss: 0.4109(0.4855) Grad: 0.5963  LR: 0.000100  Elapsed: 15m 6s (remain 6m 41s) Max mem: 834 MB\n",
      "Epoch: [1/3][4900/7000] Loss: 0.4999(0.4852) Grad: 0.8893  LR: 0.000100  Elapsed: 15m 15s (remain 6m 32s) Max mem: 834 MB\n",
      "Epoch: [1/3][4950/7000] Loss: 0.4354(0.4850) Grad: 0.6192  LR: 0.000100  Elapsed: 15m 24s (remain 6m 22s) Max mem: 834 MB\n",
      "Epoch: [1/3][5000/7000] Loss: 0.5298(0.4846) Grad: 1.0409  LR: 0.000100  Elapsed: 15m 34s (remain 6m 13s) Max mem: 834 MB\n",
      "Epoch: [1/3][5050/7000] Loss: 0.4727(0.4842) Grad: 1.4518  LR: 0.000100  Elapsed: 15m 43s (remain 6m 4s) Max mem: 834 MB\n",
      "Epoch: [1/3][5100/7000] Loss: 0.3397(0.4839) Grad: 0.6687  LR: 0.000100  Elapsed: 15m 52s (remain 5m 54s) Max mem: 834 MB\n",
      "Epoch: [1/3][5150/7000] Loss: 0.4934(0.4836) Grad: 0.9807  LR: 0.000100  Elapsed: 16m 2s (remain 5m 45s) Max mem: 834 MB\n",
      "Epoch: [1/3][5200/7000] Loss: 0.5571(0.4833) Grad: 0.8915  LR: 0.000100  Elapsed: 16m 11s (remain 5m 36s) Max mem: 834 MB\n",
      "Epoch: [1/3][5250/7000] Loss: 0.4136(0.4830) Grad: 0.5527  LR: 0.000100  Elapsed: 16m 20s (remain 5m 26s) Max mem: 834 MB\n",
      "Epoch: [1/3][5300/7000] Loss: 0.4558(0.4826) Grad: 0.7038  LR: 0.000100  Elapsed: 16m 30s (remain 5m 17s) Max mem: 834 MB\n",
      "Epoch: [1/3][5350/7000] Loss: 0.5484(0.4824) Grad: 0.8552  LR: 0.000100  Elapsed: 16m 39s (remain 5m 8s) Max mem: 834 MB\n",
      "Epoch: [1/3][5400/7000] Loss: 0.5234(0.4820) Grad: 0.7507  LR: 0.000100  Elapsed: 16m 48s (remain 4m 58s) Max mem: 834 MB\n",
      "Epoch: [1/3][5450/7000] Loss: 0.4946(0.4817) Grad: 1.2265  LR: 0.000100  Elapsed: 16m 58s (remain 4m 49s) Max mem: 834 MB\n",
      "Epoch: [1/3][5500/7000] Loss: 0.4959(0.4814) Grad: 0.6757  LR: 0.000100  Elapsed: 17m 7s (remain 4m 39s) Max mem: 834 MB\n",
      "Epoch: [1/3][5550/7000] Loss: 0.4174(0.4812) Grad: 0.8245  LR: 0.000100  Elapsed: 17m 16s (remain 4m 30s) Max mem: 834 MB\n",
      "Epoch: [1/3][5600/7000] Loss: 0.4508(0.4809) Grad: 0.9541  LR: 0.000100  Elapsed: 17m 26s (remain 4m 21s) Max mem: 834 MB\n",
      "Epoch: [1/3][5650/7000] Loss: 0.4875(0.4807) Grad: 0.8424  LR: 0.000100  Elapsed: 17m 35s (remain 4m 11s) Max mem: 834 MB\n",
      "Epoch: [1/3][5700/7000] Loss: 0.5243(0.4805) Grad: 0.8043  LR: 0.000100  Elapsed: 17m 44s (remain 4m 2s) Max mem: 834 MB\n",
      "Epoch: [1/3][5750/7000] Loss: 0.3997(0.4803) Grad: 0.7119  LR: 0.000100  Elapsed: 17m 53s (remain 3m 53s) Max mem: 834 MB\n",
      "Epoch: [1/3][5800/7000] Loss: 0.5174(0.4801) Grad: 1.0504  LR: 0.000100  Elapsed: 18m 3s (remain 3m 43s) Max mem: 834 MB\n",
      "Epoch: [1/3][5850/7000] Loss: 0.3719(0.4798) Grad: 0.5931  LR: 0.000100  Elapsed: 18m 12s (remain 3m 34s) Max mem: 834 MB\n",
      "Epoch: [1/3][5900/7000] Loss: 0.4375(0.4795) Grad: 0.6464  LR: 0.000100  Elapsed: 18m 22s (remain 3m 25s) Max mem: 834 MB\n",
      "Epoch: [1/3][5950/7000] Loss: 0.4495(0.4794) Grad: 1.2297  LR: 0.000100  Elapsed: 18m 31s (remain 3m 15s) Max mem: 834 MB\n",
      "Epoch: [1/3][6000/7000] Loss: 0.4206(0.4792) Grad: 0.7509  LR: 0.000100  Elapsed: 18m 40s (remain 3m 6s) Max mem: 834 MB\n",
      "Epoch: [1/3][6050/7000] Loss: 0.5041(0.4789) Grad: 0.9745  LR: 0.000100  Elapsed: 18m 50s (remain 2m 57s) Max mem: 834 MB\n",
      "Epoch: [1/3][6100/7000] Loss: 0.4848(0.4788) Grad: 0.9715  LR: 0.000100  Elapsed: 18m 59s (remain 2m 47s) Max mem: 834 MB\n",
      "Epoch: [1/3][6150/7000] Loss: 0.5925(0.4786) Grad: 0.9735  LR: 0.000100  Elapsed: 19m 8s (remain 2m 38s) Max mem: 834 MB\n",
      "Epoch: [1/3][6200/7000] Loss: 0.4544(0.4783) Grad: 0.8666  LR: 0.000100  Elapsed: 19m 17s (remain 2m 29s) Max mem: 834 MB\n",
      "Epoch: [1/3][6250/7000] Loss: 0.4767(0.4781) Grad: 1.3179  LR: 0.000100  Elapsed: 19m 27s (remain 2m 19s) Max mem: 834 MB\n",
      "Epoch: [1/3][6300/7000] Loss: 0.3813(0.4778) Grad: 0.5618  LR: 0.000100  Elapsed: 19m 36s (remain 2m 10s) Max mem: 834 MB\n",
      "Epoch: [1/3][6350/7000] Loss: 0.3977(0.4777) Grad: 0.9515  LR: 0.000100  Elapsed: 19m 45s (remain 2m 1s) Max mem: 834 MB\n",
      "Epoch: [1/3][6400/7000] Loss: 0.4735(0.4775) Grad: 0.6503  LR: 0.000100  Elapsed: 19m 54s (remain 1m 51s) Max mem: 834 MB\n",
      "Epoch: [1/3][6450/7000] Loss: 0.4544(0.4774) Grad: 0.8007  LR: 0.000100  Elapsed: 20m 4s (remain 1m 42s) Max mem: 834 MB\n",
      "Epoch: [1/3][6500/7000] Loss: 0.4828(0.4772) Grad: 0.7771  LR: 0.000100  Elapsed: 20m 13s (remain 1m 33s) Max mem: 834 MB\n",
      "Epoch: [1/3][6550/7000] Loss: 0.3947(0.4770) Grad: 0.7515  LR: 0.000100  Elapsed: 20m 23s (remain 1m 23s) Max mem: 834 MB\n",
      "Epoch: [1/3][6600/7000] Loss: 0.3923(0.4769) Grad: 0.9624  LR: 0.000100  Elapsed: 20m 32s (remain 1m 14s) Max mem: 834 MB\n",
      "Epoch: [1/3][6650/7000] Loss: 0.3329(0.4767) Grad: 0.7646  LR: 0.000100  Elapsed: 20m 41s (remain 1m 5s) Max mem: 834 MB\n",
      "Epoch: [1/3][6700/7000] Loss: 0.4956(0.4766) Grad: 1.0848  LR: 0.000100  Elapsed: 20m 50s (remain 0m 55s) Max mem: 834 MB\n",
      "Epoch: [1/3][6750/7000] Loss: 0.4608(0.4763) Grad: 0.6868  LR: 0.000100  Elapsed: 21m 0s (remain 0m 46s) Max mem: 834 MB\n",
      "Epoch: [1/3][6800/7000] Loss: 0.3946(0.4762) Grad: 0.7683  LR: 0.000100  Elapsed: 21m 9s (remain 0m 37s) Max mem: 834 MB\n",
      "Epoch: [1/3][6850/7000] Loss: 0.4259(0.4760) Grad: 0.7339  LR: 0.000100  Elapsed: 21m 18s (remain 0m 27s) Max mem: 834 MB\n",
      "Epoch: [1/3][6900/7000] Loss: 0.3650(0.4759) Grad: 0.6588  LR: 0.000100  Elapsed: 21m 28s (remain 0m 18s) Max mem: 834 MB\n",
      "Epoch: [1/3][6950/7000] Loss: 0.3468(0.4757) Grad: 0.5939  LR: 0.000100  Elapsed: 21m 37s (remain 0m 9s) Max mem: 834 MB\n",
      "EVAL: [0/875] Data 1.943 (1.943) Elapsed 0m 2s (remain 29m 19s) Loss: 0.4776(0.4776) \n",
      "EVAL: [50/875] Data 0.003 (0.144) Elapsed 0m 10s (remain 2m 50s) Loss: 0.4625(0.4460) \n",
      "EVAL: [100/875] Data 1.320 (0.140) Elapsed 0m 20s (remain 2m 36s) Loss: 0.5266(0.4439) \n",
      "EVAL: [150/875] Data 0.002 (0.131) Elapsed 0m 29s (remain 2m 19s) Loss: 0.4441(0.4421) \n",
      "EVAL: [200/875] Data 1.449 (0.134) Elapsed 0m 39s (remain 2m 12s) Loss: 0.4278(0.4434) \n",
      "EVAL: [250/875] Data 0.002 (0.132) Elapsed 0m 48s (remain 2m 1s) Loss: 0.3948(0.4444) \n",
      "EVAL: [300/875] Data 0.002 (0.138) Elapsed 1m 0s (remain 1m 55s) Loss: 0.4569(0.4452) \n",
      "EVAL: [350/875] Data 0.002 (0.138) Elapsed 1m 10s (remain 1m 44s) Loss: 0.4093(0.4449) \n",
      "EVAL: [400/875] Data 0.002 (0.139) Elapsed 1m 20s (remain 1m 35s) Loss: 0.4000(0.4448) \n",
      "EVAL: [450/875] Data 0.002 (0.139) Elapsed 1m 30s (remain 1m 25s) Loss: 0.3945(0.4433) \n",
      "EVAL: [500/875] Data 0.002 (0.138) Elapsed 1m 40s (remain 1m 14s) Loss: 0.4575(0.4441) \n",
      "EVAL: [550/875] Data 0.002 (0.138) Elapsed 1m 50s (remain 1m 4s) Loss: 0.4403(0.4435) \n",
      "EVAL: [600/875] Data 0.003 (0.137) Elapsed 1m 59s (remain 0m 54s) Loss: 0.4836(0.4424) \n",
      "EVAL: [650/875] Data 0.002 (0.137) Elapsed 2m 10s (remain 0m 44s) Loss: 0.4769(0.4427) \n",
      "EVAL: [700/875] Data 0.002 (0.138) Elapsed 2m 20s (remain 0m 34s) Loss: 0.4497(0.4426) \n",
      "EVAL: [750/875] Data 0.002 (0.140) Elapsed 2m 31s (remain 0m 25s) Loss: 0.4372(0.4427) \n",
      "EVAL: [800/875] Data 0.002 (0.138) Elapsed 2m 40s (remain 0m 14s) Loss: 0.5034(0.4424) \n",
      "EVAL: [850/875] Data 1.428 (0.139) Elapsed 2m 50s (remain 0m 4s) Loss: 0.4402(0.4430) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1 - avg_train_loss: 0.4756  avg_val_loss: 0.4431  time: 1480s\n",
      "Epoch 1 - Score: 0.8517\n",
      "Epoch 1 - Save Best Score: 0.8517 Model\n",
      "Epoch 1 - Save Best Loss: 0.4431 Model\n",
      "/opt/conda/lib/python3.7/site-packages/torch/optim/lr_scheduler.py:509: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  \"please use `get_last_lr()`.\", UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [2/3][0/7000] Loss: 0.5025(0.5025) Grad: 1.0671  LR: 0.000056  Elapsed: 0m 3s (remain 438m 42s) Max mem: 834 MB\n",
      "Epoch: [2/3][50/7000] Loss: 0.3946(0.4503) Grad: 0.7100  LR: 0.000056  Elapsed: 0m 14s (remain 32m 4s) Max mem: 834 MB\n",
      "Epoch: [2/3][100/7000] Loss: 0.5295(0.4431) Grad: 0.9814  LR: 0.000056  Elapsed: 0m 23s (remain 26m 32s) Max mem: 834 MB\n",
      "Epoch: [2/3][150/7000] Loss: 0.3679(0.4415) Grad: 0.7172  LR: 0.000056  Elapsed: 0m 32s (remain 24m 38s) Max mem: 834 MB\n",
      "Epoch: [2/3][200/7000] Loss: 0.4422(0.4433) Grad: 0.7781  LR: 0.000056  Elapsed: 0m 41s (remain 23m 32s) Max mem: 834 MB\n",
      "Epoch: [2/3][250/7000] Loss: 0.4588(0.4456) Grad: 0.8570  LR: 0.000056  Elapsed: 0m 51s (remain 22m 56s) Max mem: 834 MB\n",
      "Epoch: [2/3][300/7000] Loss: 0.4327(0.4462) Grad: 0.7177  LR: 0.000056  Elapsed: 1m 0s (remain 22m 25s) Max mem: 834 MB\n",
      "Epoch: [2/3][350/7000] Loss: 0.4075(0.4468) Grad: 0.5052  LR: 0.000056  Elapsed: 1m 9s (remain 22m 1s) Max mem: 834 MB\n",
      "Epoch: [2/3][400/7000] Loss: 0.5558(0.4485) Grad: 0.8751  LR: 0.000056  Elapsed: 1m 19s (remain 21m 44s) Max mem: 834 MB\n",
      "Epoch: [2/3][450/7000] Loss: 0.3970(0.4470) Grad: 0.6964  LR: 0.000056  Elapsed: 1m 28s (remain 21m 26s) Max mem: 834 MB\n",
      "Epoch: [2/3][500/7000] Loss: 0.4408(0.4466) Grad: 0.7760  LR: 0.000056  Elapsed: 1m 37s (remain 21m 9s) Max mem: 834 MB\n",
      "Epoch: [2/3][550/7000] Loss: 0.5405(0.4477) Grad: 1.3658  LR: 0.000056  Elapsed: 1m 47s (remain 20m 55s) Max mem: 834 MB\n",
      "Epoch: [2/3][600/7000] Loss: 0.4944(0.4475) Grad: 0.6762  LR: 0.000056  Elapsed: 1m 56s (remain 20m 40s) Max mem: 834 MB\n",
      "Epoch: [2/3][650/7000] Loss: 0.3912(0.4466) Grad: 0.8314  LR: 0.000056  Elapsed: 2m 5s (remain 20m 27s) Max mem: 834 MB\n",
      "Epoch: [2/3][700/7000] Loss: 0.4044(0.4471) Grad: 0.7744  LR: 0.000056  Elapsed: 2m 15s (remain 20m 16s) Max mem: 834 MB\n",
      "Epoch: [2/3][750/7000] Loss: 0.5087(0.4470) Grad: 0.7844  LR: 0.000056  Elapsed: 2m 24s (remain 20m 4s) Max mem: 834 MB\n",
      "Epoch: [2/3][800/7000] Loss: 0.4674(0.4471) Grad: 0.6390  LR: 0.000056  Elapsed: 2m 33s (remain 19m 51s) Max mem: 834 MB\n",
      "Epoch: [2/3][850/7000] Loss: 0.4757(0.4472) Grad: 1.2114  LR: 0.000056  Elapsed: 2m 43s (remain 19m 38s) Max mem: 834 MB\n",
      "Epoch: [2/3][900/7000] Loss: 0.6207(0.4471) Grad: 1.2006  LR: 0.000056  Elapsed: 2m 52s (remain 19m 28s) Max mem: 834 MB\n",
      "Epoch: [2/3][950/7000] Loss: 0.5022(0.4479) Grad: 1.0421  LR: 0.000056  Elapsed: 3m 1s (remain 19m 16s) Max mem: 834 MB\n",
      "Epoch: [2/3][1000/7000] Loss: 0.4123(0.4474) Grad: 0.7749  LR: 0.000056  Elapsed: 3m 11s (remain 19m 5s) Max mem: 834 MB\n",
      "Epoch: [2/3][1050/7000] Loss: 0.4839(0.4469) Grad: 0.9418  LR: 0.000056  Elapsed: 3m 20s (remain 18m 54s) Max mem: 834 MB\n",
      "Epoch: [2/3][1100/7000] Loss: 0.4587(0.4468) Grad: 0.8432  LR: 0.000056  Elapsed: 3m 29s (remain 18m 43s) Max mem: 834 MB\n",
      "Epoch: [2/3][1150/7000] Loss: 0.3613(0.4468) Grad: 0.6522  LR: 0.000056  Elapsed: 3m 38s (remain 18m 32s) Max mem: 834 MB\n",
      "Epoch: [2/3][1200/7000] Loss: 0.4967(0.4467) Grad: 0.9411  LR: 0.000056  Elapsed: 3m 48s (remain 18m 22s) Max mem: 834 MB\n",
      "Epoch: [2/3][1250/7000] Loss: 0.4807(0.4464) Grad: 0.6968  LR: 0.000056  Elapsed: 3m 57s (remain 18m 12s) Max mem: 834 MB\n",
      "Epoch: [2/3][1300/7000] Loss: 0.4109(0.4459) Grad: 0.7740  LR: 0.000056  Elapsed: 4m 7s (remain 18m 2s) Max mem: 834 MB\n",
      "Epoch: [2/3][1350/7000] Loss: 0.5209(0.4462) Grad: 0.8444  LR: 0.000056  Elapsed: 4m 16s (remain 17m 51s) Max mem: 834 MB\n",
      "Epoch: [2/3][1400/7000] Loss: 0.4057(0.4459) Grad: 0.9313  LR: 0.000056  Elapsed: 4m 25s (remain 17m 42s) Max mem: 834 MB\n",
      "Epoch: [2/3][1450/7000] Loss: 0.4709(0.4461) Grad: 0.6686  LR: 0.000056  Elapsed: 4m 34s (remain 17m 31s) Max mem: 834 MB\n",
      "Epoch: [2/3][1500/7000] Loss: 0.4651(0.4463) Grad: 0.6829  LR: 0.000056  Elapsed: 4m 44s (remain 17m 21s) Max mem: 834 MB\n",
      "Epoch: [2/3][1550/7000] Loss: 0.4925(0.4463) Grad: 0.8946  LR: 0.000056  Elapsed: 4m 53s (remain 17m 12s) Max mem: 834 MB\n",
      "Epoch: [2/3][1600/7000] Loss: 0.4636(0.4462) Grad: 0.8463  LR: 0.000056  Elapsed: 5m 3s (remain 17m 2s) Max mem: 834 MB\n",
      "Epoch: [2/3][1650/7000] Loss: 0.5395(0.4465) Grad: 0.7883  LR: 0.000056  Elapsed: 5m 12s (remain 16m 51s) Max mem: 834 MB\n",
      "Epoch: [2/3][1700/7000] Loss: 0.4116(0.4462) Grad: 0.8604  LR: 0.000056  Elapsed: 5m 21s (remain 16m 41s) Max mem: 834 MB\n",
      "Epoch: [2/3][1750/7000] Loss: 0.4571(0.4460) Grad: 0.9402  LR: 0.000056  Elapsed: 5m 30s (remain 16m 31s) Max mem: 834 MB\n",
      "Epoch: [2/3][1800/7000] Loss: 0.5036(0.4463) Grad: 0.7867  LR: 0.000056  Elapsed: 5m 40s (remain 16m 21s) Max mem: 834 MB\n",
      "Epoch: [2/3][1850/7000] Loss: 0.4349(0.4461) Grad: 1.0843  LR: 0.000056  Elapsed: 5m 49s (remain 16m 11s) Max mem: 834 MB\n",
      "Epoch: [2/3][1900/7000] Loss: 0.4683(0.4461) Grad: 0.9591  LR: 0.000056  Elapsed: 5m 58s (remain 16m 1s) Max mem: 834 MB\n",
      "Epoch: [2/3][1950/7000] Loss: 0.4800(0.4457) Grad: 0.9338  LR: 0.000056  Elapsed: 6m 7s (remain 15m 51s) Max mem: 834 MB\n",
      "Epoch: [2/3][2000/7000] Loss: 0.4082(0.4457) Grad: 0.7372  LR: 0.000056  Elapsed: 6m 17s (remain 15m 42s) Max mem: 834 MB\n",
      "Epoch: [2/3][2050/7000] Loss: 0.3915(0.4454) Grad: 0.9465  LR: 0.000056  Elapsed: 6m 26s (remain 15m 32s) Max mem: 834 MB\n",
      "Epoch: [2/3][2100/7000] Loss: 0.3706(0.4454) Grad: 0.7839  LR: 0.000056  Elapsed: 6m 35s (remain 15m 22s) Max mem: 834 MB\n",
      "Epoch: [2/3][2150/7000] Loss: 0.4762(0.4454) Grad: 0.7809  LR: 0.000056  Elapsed: 6m 44s (remain 15m 12s) Max mem: 834 MB\n",
      "Epoch: [2/3][2200/7000] Loss: 0.5528(0.4454) Grad: 0.9005  LR: 0.000056  Elapsed: 6m 54s (remain 15m 3s) Max mem: 834 MB\n",
      "Epoch: [2/3][2250/7000] Loss: 0.4754(0.4456) Grad: 0.5759  LR: 0.000056  Elapsed: 7m 3s (remain 14m 53s) Max mem: 834 MB\n",
      "Epoch: [2/3][2300/7000] Loss: 0.3760(0.4457) Grad: 0.8157  LR: 0.000056  Elapsed: 7m 13s (remain 14m 44s) Max mem: 834 MB\n",
      "Epoch: [2/3][2350/7000] Loss: 0.5424(0.4455) Grad: 0.9162  LR: 0.000056  Elapsed: 7m 22s (remain 14m 34s) Max mem: 834 MB\n",
      "Epoch: [2/3][2400/7000] Loss: 0.4708(0.4455) Grad: 0.5963  LR: 0.000056  Elapsed: 7m 31s (remain 14m 24s) Max mem: 834 MB\n",
      "Epoch: [2/3][2450/7000] Loss: 0.4171(0.4456) Grad: 0.6651  LR: 0.000056  Elapsed: 7m 40s (remain 14m 15s) Max mem: 834 MB\n",
      "Epoch: [2/3][2500/7000] Loss: 0.4926(0.4455) Grad: 1.0111  LR: 0.000056  Elapsed: 7m 49s (remain 14m 5s) Max mem: 834 MB\n",
      "Epoch: [2/3][2550/7000] Loss: 0.4309(0.4455) Grad: 0.8257  LR: 0.000056  Elapsed: 7m 59s (remain 13m 56s) Max mem: 834 MB\n",
      "Epoch: [2/3][2600/7000] Loss: 0.3139(0.4454) Grad: 0.9924  LR: 0.000056  Elapsed: 8m 8s (remain 13m 46s) Max mem: 834 MB\n",
      "Epoch: [2/3][2650/7000] Loss: 0.3882(0.4454) Grad: 0.6790  LR: 0.000056  Elapsed: 8m 18s (remain 13m 37s) Max mem: 834 MB\n",
      "Epoch: [2/3][2700/7000] Loss: 0.5669(0.4455) Grad: 0.9304  LR: 0.000056  Elapsed: 8m 27s (remain 13m 27s) Max mem: 834 MB\n",
      "Epoch: [2/3][2750/7000] Loss: 0.4129(0.4458) Grad: 0.6673  LR: 0.000056  Elapsed: 8m 36s (remain 13m 18s) Max mem: 834 MB\n",
      "Epoch: [2/3][2800/7000] Loss: 0.3962(0.4455) Grad: 0.9864  LR: 0.000056  Elapsed: 8m 45s (remain 13m 8s) Max mem: 834 MB\n",
      "Epoch: [2/3][2850/7000] Loss: 0.4096(0.4455) Grad: 1.0584  LR: 0.000056  Elapsed: 8m 55s (remain 12m 58s) Max mem: 834 MB\n",
      "Epoch: [2/3][2900/7000] Loss: 0.4479(0.4455) Grad: 0.9740  LR: 0.000056  Elapsed: 9m 4s (remain 12m 49s) Max mem: 834 MB\n",
      "Epoch: [2/3][2950/7000] Loss: 0.4504(0.4456) Grad: 0.7310  LR: 0.000056  Elapsed: 9m 13s (remain 12m 39s) Max mem: 834 MB\n",
      "Epoch: [2/3][3000/7000] Loss: 0.4518(0.4457) Grad: 0.9579  LR: 0.000056  Elapsed: 9m 22s (remain 12m 30s) Max mem: 834 MB\n",
      "Epoch: [2/3][3050/7000] Loss: 0.3684(0.4457) Grad: 0.9572  LR: 0.000056  Elapsed: 9m 32s (remain 12m 20s) Max mem: 834 MB\n",
      "Epoch: [2/3][3100/7000] Loss: 0.4003(0.4457) Grad: 0.6580  LR: 0.000056  Elapsed: 9m 41s (remain 12m 11s) Max mem: 834 MB\n",
      "Epoch: [2/3][3150/7000] Loss: 0.4181(0.4459) Grad: 0.9200  LR: 0.000056  Elapsed: 9m 50s (remain 12m 1s) Max mem: 834 MB\n",
      "Epoch: [2/3][3200/7000] Loss: 0.3508(0.4456) Grad: 0.7827  LR: 0.000056  Elapsed: 10m 0s (remain 11m 52s) Max mem: 834 MB\n",
      "Epoch: [2/3][3250/7000] Loss: 0.4512(0.4457) Grad: 1.0266  LR: 0.000056  Elapsed: 10m 9s (remain 11m 42s) Max mem: 834 MB\n",
      "Epoch: [2/3][3300/7000] Loss: 0.4327(0.4455) Grad: 0.8154  LR: 0.000056  Elapsed: 10m 18s (remain 11m 33s) Max mem: 834 MB\n",
      "Epoch: [2/3][3350/7000] Loss: 0.4086(0.4457) Grad: 0.6992  LR: 0.000056  Elapsed: 10m 28s (remain 11m 24s) Max mem: 834 MB\n",
      "Epoch: [2/3][3400/7000] Loss: 0.3366(0.4455) Grad: 0.7006  LR: 0.000056  Elapsed: 10m 37s (remain 11m 14s) Max mem: 834 MB\n",
      "Epoch: [2/3][3450/7000] Loss: 0.4097(0.4455) Grad: 0.8361  LR: 0.000056  Elapsed: 10m 46s (remain 11m 5s) Max mem: 834 MB\n",
      "Epoch: [2/3][3500/7000] Loss: 0.4783(0.4456) Grad: 0.7930  LR: 0.000056  Elapsed: 10m 56s (remain 10m 55s) Max mem: 834 MB\n",
      "Epoch: [2/3][3550/7000] Loss: 0.3876(0.4457) Grad: 0.7640  LR: 0.000056  Elapsed: 11m 5s (remain 10m 46s) Max mem: 834 MB\n",
      "Epoch: [2/3][3600/7000] Loss: 0.4741(0.4455) Grad: 0.8694  LR: 0.000056  Elapsed: 11m 14s (remain 10m 36s) Max mem: 834 MB\n",
      "Epoch: [2/3][3650/7000] Loss: 0.4910(0.4452) Grad: 1.1432  LR: 0.000056  Elapsed: 11m 24s (remain 10m 27s) Max mem: 834 MB\n",
      "Epoch: [2/3][3700/7000] Loss: 0.4505(0.4453) Grad: 0.8510  LR: 0.000056  Elapsed: 11m 33s (remain 10m 18s) Max mem: 834 MB\n",
      "Epoch: [2/3][3750/7000] Loss: 0.5286(0.4451) Grad: 1.0131  LR: 0.000056  Elapsed: 11m 42s (remain 10m 8s) Max mem: 834 MB\n",
      "Epoch: [2/3][3800/7000] Loss: 0.5841(0.4452) Grad: 1.3179  LR: 0.000056  Elapsed: 11m 52s (remain 9m 59s) Max mem: 834 MB\n",
      "Epoch: [2/3][3850/7000] Loss: 0.5232(0.4450) Grad: 1.1532  LR: 0.000056  Elapsed: 12m 1s (remain 9m 50s) Max mem: 834 MB\n",
      "Epoch: [2/3][3900/7000] Loss: 0.4748(0.4450) Grad: 1.0706  LR: 0.000056  Elapsed: 12m 11s (remain 9m 40s) Max mem: 834 MB\n",
      "Epoch: [2/3][3950/7000] Loss: 0.4029(0.4450) Grad: 0.7589  LR: 0.000056  Elapsed: 12m 20s (remain 9m 31s) Max mem: 834 MB\n",
      "Epoch: [2/3][4000/7000] Loss: 0.5056(0.4449) Grad: 0.9735  LR: 0.000056  Elapsed: 12m 29s (remain 9m 22s) Max mem: 834 MB\n",
      "Epoch: [2/3][4050/7000] Loss: 0.3809(0.4446) Grad: 0.7857  LR: 0.000056  Elapsed: 12m 39s (remain 9m 12s) Max mem: 834 MB\n",
      "Epoch: [2/3][4100/7000] Loss: 0.4971(0.4446) Grad: 1.0949  LR: 0.000056  Elapsed: 12m 48s (remain 9m 3s) Max mem: 834 MB\n",
      "Epoch: [2/3][4150/7000] Loss: 0.4048(0.4445) Grad: 1.0796  LR: 0.000056  Elapsed: 12m 58s (remain 8m 54s) Max mem: 834 MB\n",
      "Epoch: [2/3][4200/7000] Loss: 0.4723(0.4445) Grad: 0.8829  LR: 0.000056  Elapsed: 13m 7s (remain 8m 44s) Max mem: 834 MB\n",
      "Epoch: [2/3][4250/7000] Loss: 0.3347(0.4446) Grad: 0.7640  LR: 0.000056  Elapsed: 13m 16s (remain 8m 35s) Max mem: 834 MB\n",
      "Epoch: [2/3][4300/7000] Loss: 0.4592(0.4445) Grad: 1.3527  LR: 0.000056  Elapsed: 13m 25s (remain 8m 25s) Max mem: 834 MB\n",
      "Epoch: [2/3][4350/7000] Loss: 0.4737(0.4446) Grad: 1.2065  LR: 0.000056  Elapsed: 13m 35s (remain 8m 16s) Max mem: 834 MB\n",
      "Epoch: [2/3][4400/7000] Loss: 0.4621(0.4446) Grad: 0.9689  LR: 0.000056  Elapsed: 13m 44s (remain 8m 6s) Max mem: 834 MB\n",
      "Epoch: [2/3][4450/7000] Loss: 0.4040(0.4448) Grad: 0.6611  LR: 0.000056  Elapsed: 13m 54s (remain 7m 57s) Max mem: 834 MB\n",
      "Epoch: [2/3][4500/7000] Loss: 0.4106(0.4446) Grad: 0.7101  LR: 0.000056  Elapsed: 14m 3s (remain 7m 48s) Max mem: 834 MB\n",
      "Epoch: [2/3][4550/7000] Loss: 0.4350(0.4445) Grad: 1.1396  LR: 0.000056  Elapsed: 14m 12s (remain 7m 38s) Max mem: 834 MB\n",
      "Epoch: [2/3][4600/7000] Loss: 0.4478(0.4447) Grad: 0.6763  LR: 0.000056  Elapsed: 14m 21s (remain 7m 29s) Max mem: 834 MB\n",
      "Epoch: [2/3][4650/7000] Loss: 0.5083(0.4449) Grad: 0.7982  LR: 0.000056  Elapsed: 14m 30s (remain 7m 19s) Max mem: 834 MB\n",
      "Epoch: [2/3][4700/7000] Loss: 0.4796(0.4448) Grad: 1.0950  LR: 0.000056  Elapsed: 14m 40s (remain 7m 10s) Max mem: 834 MB\n",
      "Epoch: [2/3][4750/7000] Loss: 0.4136(0.4449) Grad: 1.0099  LR: 0.000056  Elapsed: 14m 49s (remain 7m 1s) Max mem: 834 MB\n",
      "Epoch: [2/3][4800/7000] Loss: 0.3755(0.4448) Grad: 0.7725  LR: 0.000056  Elapsed: 14m 59s (remain 6m 51s) Max mem: 834 MB\n",
      "Epoch: [2/3][4850/7000] Loss: 0.4123(0.4448) Grad: 0.8098  LR: 0.000056  Elapsed: 15m 8s (remain 6m 42s) Max mem: 834 MB\n",
      "Epoch: [2/3][4900/7000] Loss: 0.5293(0.4447) Grad: 1.3418  LR: 0.000056  Elapsed: 15m 17s (remain 6m 33s) Max mem: 834 MB\n",
      "Epoch: [2/3][4950/7000] Loss: 0.5124(0.4447) Grad: 1.0668  LR: 0.000056  Elapsed: 15m 27s (remain 6m 23s) Max mem: 834 MB\n",
      "Epoch: [2/3][5000/7000] Loss: 0.4402(0.4447) Grad: 0.6870  LR: 0.000056  Elapsed: 15m 36s (remain 6m 14s) Max mem: 834 MB\n",
      "Epoch: [2/3][5050/7000] Loss: 0.3746(0.4445) Grad: 0.8511  LR: 0.000056  Elapsed: 15m 45s (remain 6m 4s) Max mem: 834 MB\n",
      "Epoch: [2/3][5100/7000] Loss: 0.4039(0.4445) Grad: 0.7671  LR: 0.000056  Elapsed: 15m 55s (remain 5m 55s) Max mem: 834 MB\n",
      "Epoch: [2/3][5150/7000] Loss: 0.4455(0.4444) Grad: 0.9291  LR: 0.000056  Elapsed: 16m 4s (remain 5m 46s) Max mem: 834 MB\n",
      "Epoch: [2/3][5200/7000] Loss: 0.3800(0.4443) Grad: 0.8401  LR: 0.000056  Elapsed: 16m 13s (remain 5m 36s) Max mem: 834 MB\n",
      "Epoch: [2/3][5250/7000] Loss: 0.4327(0.4443) Grad: 0.7339  LR: 0.000056  Elapsed: 16m 22s (remain 5m 27s) Max mem: 834 MB\n",
      "Epoch: [2/3][5300/7000] Loss: 0.4143(0.4442) Grad: 0.8367  LR: 0.000056  Elapsed: 16m 32s (remain 5m 18s) Max mem: 834 MB\n",
      "Epoch: [2/3][5350/7000] Loss: 0.3219(0.4441) Grad: 0.9257  LR: 0.000056  Elapsed: 16m 41s (remain 5m 8s) Max mem: 834 MB\n",
      "Epoch: [2/3][5400/7000] Loss: 0.3397(0.4440) Grad: 0.7764  LR: 0.000056  Elapsed: 16m 50s (remain 4m 59s) Max mem: 834 MB\n",
      "Epoch: [2/3][5450/7000] Loss: 0.3851(0.4440) Grad: 0.7802  LR: 0.000056  Elapsed: 17m 0s (remain 4m 49s) Max mem: 834 MB\n",
      "Epoch: [2/3][5500/7000] Loss: 0.4685(0.4439) Grad: 0.8162  LR: 0.000056  Elapsed: 17m 10s (remain 4m 40s) Max mem: 834 MB\n",
      "Epoch: [2/3][5550/7000] Loss: 0.4641(0.4439) Grad: 0.8435  LR: 0.000056  Elapsed: 17m 19s (remain 4m 31s) Max mem: 834 MB\n",
      "Epoch: [2/3][5600/7000] Loss: 0.4810(0.4438) Grad: 1.4468  LR: 0.000056  Elapsed: 17m 29s (remain 4m 22s) Max mem: 834 MB\n",
      "Epoch: [2/3][5650/7000] Loss: 0.4384(0.4438) Grad: 0.6033  LR: 0.000056  Elapsed: 17m 38s (remain 4m 12s) Max mem: 834 MB\n",
      "Epoch: [2/3][5700/7000] Loss: 0.4215(0.4439) Grad: 0.7053  LR: 0.000056  Elapsed: 17m 48s (remain 4m 3s) Max mem: 834 MB\n",
      "Epoch: [2/3][5750/7000] Loss: 0.4009(0.4438) Grad: 0.9520  LR: 0.000056  Elapsed: 17m 57s (remain 3m 54s) Max mem: 834 MB\n",
      "Epoch: [2/3][5800/7000] Loss: 0.4461(0.4437) Grad: 0.8532  LR: 0.000056  Elapsed: 18m 7s (remain 3m 44s) Max mem: 834 MB\n",
      "Epoch: [2/3][5850/7000] Loss: 0.5433(0.4438) Grad: 0.9234  LR: 0.000056  Elapsed: 18m 16s (remain 3m 35s) Max mem: 834 MB\n",
      "Epoch: [2/3][5900/7000] Loss: 0.4132(0.4439) Grad: 0.8881  LR: 0.000056  Elapsed: 18m 25s (remain 3m 25s) Max mem: 834 MB\n",
      "Epoch: [2/3][5950/7000] Loss: 0.5121(0.4439) Grad: 1.1884  LR: 0.000056  Elapsed: 18m 35s (remain 3m 16s) Max mem: 834 MB\n",
      "Epoch: [2/3][6000/7000] Loss: 0.4110(0.4438) Grad: 0.8315  LR: 0.000056  Elapsed: 18m 44s (remain 3m 7s) Max mem: 834 MB\n",
      "Epoch: [2/3][6050/7000] Loss: 0.4189(0.4439) Grad: 0.8693  LR: 0.000056  Elapsed: 18m 53s (remain 2m 57s) Max mem: 834 MB\n",
      "Epoch: [2/3][6100/7000] Loss: 0.4445(0.4439) Grad: 0.9975  LR: 0.000056  Elapsed: 19m 3s (remain 2m 48s) Max mem: 834 MB\n",
      "Epoch: [2/3][6150/7000] Loss: 0.4360(0.4438) Grad: 1.0864  LR: 0.000056  Elapsed: 19m 12s (remain 2m 39s) Max mem: 834 MB\n",
      "Epoch: [2/3][6200/7000] Loss: 0.4348(0.4438) Grad: 0.8057  LR: 0.000056  Elapsed: 19m 21s (remain 2m 29s) Max mem: 834 MB\n",
      "Epoch: [2/3][6250/7000] Loss: 0.3473(0.4438) Grad: 0.6692  LR: 0.000056  Elapsed: 19m 31s (remain 2m 20s) Max mem: 834 MB\n",
      "Epoch: [2/3][6300/7000] Loss: 0.3871(0.4437) Grad: 0.8322  LR: 0.000056  Elapsed: 19m 40s (remain 2m 10s) Max mem: 834 MB\n",
      "Epoch: [2/3][6350/7000] Loss: 0.4210(0.4437) Grad: 0.8346  LR: 0.000056  Elapsed: 19m 49s (remain 2m 1s) Max mem: 834 MB\n",
      "Epoch: [2/3][6400/7000] Loss: 0.4157(0.4437) Grad: 0.7558  LR: 0.000056  Elapsed: 19m 59s (remain 1m 52s) Max mem: 834 MB\n",
      "Epoch: [2/3][6450/7000] Loss: 0.4195(0.4438) Grad: 0.9192  LR: 0.000056  Elapsed: 20m 8s (remain 1m 42s) Max mem: 834 MB\n",
      "Epoch: [2/3][6500/7000] Loss: 0.5206(0.4438) Grad: 1.3640  LR: 0.000056  Elapsed: 20m 18s (remain 1m 33s) Max mem: 834 MB\n",
      "Epoch: [2/3][6550/7000] Loss: 0.4544(0.4439) Grad: 1.4817  LR: 0.000056  Elapsed: 20m 27s (remain 1m 24s) Max mem: 834 MB\n",
      "Epoch: [2/3][6600/7000] Loss: 0.4824(0.4439) Grad: 0.8799  LR: 0.000056  Elapsed: 20m 36s (remain 1m 14s) Max mem: 834 MB\n",
      "Epoch: [2/3][6650/7000] Loss: 0.3622(0.4438) Grad: 0.6969  LR: 0.000056  Elapsed: 20m 46s (remain 1m 5s) Max mem: 834 MB\n",
      "Epoch: [2/3][6700/7000] Loss: 0.4521(0.4438) Grad: 0.7435  LR: 0.000056  Elapsed: 20m 55s (remain 0m 56s) Max mem: 834 MB\n",
      "Epoch: [2/3][6750/7000] Loss: 0.5126(0.4438) Grad: 1.0062  LR: 0.000056  Elapsed: 21m 4s (remain 0m 46s) Max mem: 834 MB\n",
      "Epoch: [2/3][6800/7000] Loss: 0.4302(0.4438) Grad: 0.6932  LR: 0.000056  Elapsed: 21m 13s (remain 0m 37s) Max mem: 834 MB\n",
      "Epoch: [2/3][6850/7000] Loss: 0.3230(0.4438) Grad: 0.7163  LR: 0.000056  Elapsed: 21m 23s (remain 0m 27s) Max mem: 834 MB\n",
      "Epoch: [2/3][6900/7000] Loss: 0.4010(0.4437) Grad: 0.9927  LR: 0.000056  Elapsed: 21m 32s (remain 0m 18s) Max mem: 834 MB\n",
      "Epoch: [2/3][6950/7000] Loss: 0.4431(0.4437) Grad: 0.7913  LR: 0.000056  Elapsed: 21m 41s (remain 0m 9s) Max mem: 834 MB\n",
      "EVAL: [0/875] Data 1.659 (1.659) Elapsed 0m 1s (remain 25m 8s) Loss: 0.4840(0.4840) \n",
      "EVAL: [50/875] Data 0.002 (0.143) Elapsed 0m 10s (remain 2m 49s) Loss: 0.4842(0.4440) \n",
      "EVAL: [100/875] Data 1.334 (0.138) Elapsed 0m 20s (remain 2m 35s) Loss: 0.5268(0.4420) \n",
      "EVAL: [150/875] Data 0.002 (0.138) Elapsed 0m 30s (remain 2m 25s) Loss: 0.4325(0.4386) \n",
      "EVAL: [200/875] Data 1.367 (0.142) Elapsed 0m 40s (remain 2m 17s) Loss: 0.4225(0.4409) \n",
      "EVAL: [250/875] Data 0.002 (0.136) Elapsed 0m 49s (remain 2m 3s) Loss: 0.3840(0.4419) \n",
      "EVAL: [300/875] Data 0.002 (0.133) Elapsed 0m 58s (remain 1m 51s) Loss: 0.4601(0.4424) \n",
      "EVAL: [350/875] Data 0.002 (0.133) Elapsed 1m 8s (remain 1m 42s) Loss: 0.3991(0.4418) \n",
      "EVAL: [400/875] Data 0.002 (0.131) Elapsed 1m 17s (remain 1m 31s) Loss: 0.4085(0.4416) \n",
      "EVAL: [450/875] Data 0.002 (0.130) Elapsed 1m 26s (remain 1m 21s) Loss: 0.3789(0.4401) \n",
      "EVAL: [500/875] Data 0.002 (0.128) Elapsed 1m 35s (remain 1m 10s) Loss: 0.4531(0.4411) \n",
      "EVAL: [550/875] Data 0.002 (0.129) Elapsed 1m 45s (remain 1m 1s) Loss: 0.4266(0.4403) \n",
      "EVAL: [600/875] Data 0.002 (0.128) Elapsed 1m 53s (remain 0m 51s) Loss: 0.4863(0.4391) \n",
      "EVAL: [650/875] Data 0.002 (0.129) Elapsed 2m 4s (remain 0m 42s) Loss: 0.4662(0.4394) \n",
      "EVAL: [700/875] Data 0.002 (0.127) Elapsed 2m 12s (remain 0m 32s) Loss: 0.4578(0.4394) \n",
      "EVAL: [750/875] Data 0.002 (0.128) Elapsed 2m 22s (remain 0m 23s) Loss: 0.4121(0.4396) \n",
      "EVAL: [800/875] Data 0.002 (0.127) Elapsed 2m 31s (remain 0m 13s) Loss: 0.5124(0.4392) \n",
      "EVAL: [850/875] Data 1.418 (0.127) Elapsed 2m 41s (remain 0m 4s) Loss: 0.4239(0.4398) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2 - avg_train_loss: 0.4437  avg_val_loss: 0.4399  time: 1475s\n",
      "Epoch 2 - Score: 0.8554\n",
      "Epoch 2 - Save Best Score: 0.8554 Model\n",
      "Epoch 2 - Save Best Loss: 0.4399 Model\n",
      "/opt/conda/lib/python3.7/site-packages/torch/optim/lr_scheduler.py:509: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  \"please use `get_last_lr()`.\", UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [3/3][0/7000] Loss: 0.5196(0.5196) Grad: 1.1236  LR: 0.000008  Elapsed: 0m 3s (remain 414m 12s) Max mem: 834 MB\n",
      "Epoch: [3/3][50/7000] Loss: 0.4638(0.4477) Grad: 0.7317  LR: 0.000008  Elapsed: 0m 13s (remain 29m 50s) Max mem: 834 MB\n",
      "Epoch: [3/3][100/7000] Loss: 0.4760(0.4385) Grad: 0.9307  LR: 0.000008  Elapsed: 0m 22s (remain 25m 26s) Max mem: 834 MB\n",
      "Epoch: [3/3][150/7000] Loss: 0.5032(0.4332) Grad: 1.2528  LR: 0.000008  Elapsed: 0m 31s (remain 23m 45s) Max mem: 834 MB\n",
      "Epoch: [3/3][200/7000] Loss: 0.4255(0.4341) Grad: 0.8871  LR: 0.000008  Elapsed: 0m 40s (remain 22m 57s) Max mem: 834 MB\n",
      "Epoch: [3/3][250/7000] Loss: 0.4152(0.4372) Grad: 0.8828  LR: 0.000008  Elapsed: 0m 50s (remain 22m 27s) Max mem: 834 MB\n",
      "Epoch: [3/3][300/7000] Loss: 0.4336(0.4363) Grad: 1.1955  LR: 0.000008  Elapsed: 0m 59s (remain 22m 0s) Max mem: 834 MB\n",
      "Epoch: [3/3][350/7000] Loss: 0.3742(0.4380) Grad: 0.8685  LR: 0.000008  Elapsed: 1m 8s (remain 21m 38s) Max mem: 834 MB\n",
      "Epoch: [3/3][400/7000] Loss: 0.4514(0.4386) Grad: 1.4011  LR: 0.000008  Elapsed: 1m 18s (remain 21m 24s) Max mem: 834 MB\n",
      "Epoch: [3/3][450/7000] Loss: 0.4433(0.4378) Grad: 0.9879  LR: 0.000008  Elapsed: 1m 27s (remain 21m 9s) Max mem: 834 MB\n",
      "Epoch: [3/3][500/7000] Loss: 0.4566(0.4370) Grad: 1.4281  LR: 0.000008  Elapsed: 1m 36s (remain 20m 52s) Max mem: 834 MB\n",
      "Epoch: [3/3][550/7000] Loss: 0.4610(0.4380) Grad: 0.9058  LR: 0.000008  Elapsed: 1m 45s (remain 20m 39s) Max mem: 834 MB\n",
      "Epoch: [3/3][600/7000] Loss: 0.3947(0.4376) Grad: 1.0809  LR: 0.000008  Elapsed: 1m 55s (remain 20m 25s) Max mem: 834 MB\n",
      "Epoch: [3/3][650/7000] Loss: 0.4544(0.4373) Grad: 0.8194  LR: 0.000008  Elapsed: 2m 4s (remain 20m 13s) Max mem: 834 MB\n",
      "Epoch: [3/3][700/7000] Loss: 0.3840(0.4375) Grad: 0.7013  LR: 0.000008  Elapsed: 2m 13s (remain 20m 2s) Max mem: 834 MB\n",
      "Epoch: [3/3][750/7000] Loss: 0.4737(0.4372) Grad: 0.9563  LR: 0.000008  Elapsed: 2m 22s (remain 19m 48s) Max mem: 834 MB\n",
      "Epoch: [3/3][800/7000] Loss: 0.3983(0.4373) Grad: 1.0556  LR: 0.000008  Elapsed: 2m 32s (remain 19m 37s) Max mem: 834 MB\n",
      "Epoch: [3/3][850/7000] Loss: 0.4050(0.4367) Grad: 0.8426  LR: 0.000008  Elapsed: 2m 41s (remain 19m 25s) Max mem: 834 MB\n",
      "Epoch: [3/3][900/7000] Loss: 0.4650(0.4368) Grad: 1.0083  LR: 0.000008  Elapsed: 2m 50s (remain 19m 16s) Max mem: 834 MB\n",
      "Epoch: [3/3][950/7000] Loss: 0.3728(0.4372) Grad: 0.8056  LR: 0.000008  Elapsed: 3m 0s (remain 19m 5s) Max mem: 834 MB\n",
      "Epoch: [3/3][1000/7000] Loss: 0.4959(0.4369) Grad: 1.0040  LR: 0.000008  Elapsed: 3m 9s (remain 18m 54s) Max mem: 834 MB\n",
      "Epoch: [3/3][1050/7000] Loss: 0.3702(0.4369) Grad: 0.9414  LR: 0.000008  Elapsed: 3m 18s (remain 18m 45s) Max mem: 834 MB\n",
      "Epoch: [3/3][1100/7000] Loss: 0.4661(0.4359) Grad: 1.4199  LR: 0.000008  Elapsed: 3m 28s (remain 18m 35s) Max mem: 834 MB\n",
      "Epoch: [3/3][1150/7000] Loss: 0.4182(0.4363) Grad: 0.9483  LR: 0.000008  Elapsed: 3m 37s (remain 18m 24s) Max mem: 834 MB\n",
      "Epoch: [3/3][1200/7000] Loss: 0.4594(0.4365) Grad: 1.3628  LR: 0.000008  Elapsed: 3m 46s (remain 18m 14s) Max mem: 834 MB\n",
      "Epoch: [3/3][1250/7000] Loss: 0.3453(0.4364) Grad: 0.7675  LR: 0.000008  Elapsed: 3m 56s (remain 18m 4s) Max mem: 834 MB\n",
      "Epoch: [3/3][1300/7000] Loss: 0.3922(0.4365) Grad: 0.8361  LR: 0.000008  Elapsed: 4m 5s (remain 17m 53s) Max mem: 834 MB\n",
      "Epoch: [3/3][1350/7000] Loss: 0.5081(0.4363) Grad: 1.5048  LR: 0.000008  Elapsed: 4m 14s (remain 17m 43s) Max mem: 834 MB\n",
      "Epoch: [3/3][1400/7000] Loss: 0.4397(0.4361) Grad: 1.2498  LR: 0.000008  Elapsed: 4m 23s (remain 17m 34s) Max mem: 834 MB\n",
      "Epoch: [3/3][1450/7000] Loss: 0.4900(0.4366) Grad: 1.0924  LR: 0.000008  Elapsed: 4m 33s (remain 17m 24s) Max mem: 834 MB\n",
      "Epoch: [3/3][1500/7000] Loss: 0.5436(0.4366) Grad: 1.1006  LR: 0.000008  Elapsed: 4m 42s (remain 17m 13s) Max mem: 834 MB\n",
      "Epoch: [3/3][1550/7000] Loss: 0.4330(0.4365) Grad: 0.9315  LR: 0.000008  Elapsed: 4m 51s (remain 17m 3s) Max mem: 834 MB\n",
      "Epoch: [3/3][1600/7000] Loss: 0.4921(0.4364) Grad: 1.2554  LR: 0.000008  Elapsed: 5m 0s (remain 16m 53s) Max mem: 834 MB\n",
      "Epoch: [3/3][1650/7000] Loss: 0.3045(0.4363) Grad: 0.8285  LR: 0.000008  Elapsed: 5m 10s (remain 16m 45s) Max mem: 834 MB\n",
      "Epoch: [3/3][1700/7000] Loss: 0.4233(0.4365) Grad: 1.0087  LR: 0.000008  Elapsed: 5m 19s (remain 16m 34s) Max mem: 834 MB\n",
      "Epoch: [3/3][1750/7000] Loss: 0.3894(0.4365) Grad: 0.9388  LR: 0.000008  Elapsed: 5m 28s (remain 16m 25s) Max mem: 834 MB\n",
      "Epoch: [3/3][1800/7000] Loss: 0.3346(0.4363) Grad: 0.8614  LR: 0.000008  Elapsed: 5m 37s (remain 16m 15s) Max mem: 834 MB\n",
      "Epoch: [3/3][1850/7000] Loss: 0.4102(0.4363) Grad: 1.0457  LR: 0.000008  Elapsed: 5m 46s (remain 16m 4s) Max mem: 834 MB\n",
      "Epoch: [3/3][1900/7000] Loss: 0.4842(0.4362) Grad: 0.9930  LR: 0.000008  Elapsed: 5m 56s (remain 15m 55s) Max mem: 834 MB\n",
      "Epoch: [3/3][1950/7000] Loss: 0.3552(0.4359) Grad: 0.8979  LR: 0.000008  Elapsed: 6m 5s (remain 15m 45s) Max mem: 834 MB\n",
      "Epoch: [3/3][2000/7000] Loss: 0.4649(0.4358) Grad: 1.2582  LR: 0.000008  Elapsed: 6m 14s (remain 15m 35s) Max mem: 834 MB\n",
      "Epoch: [3/3][2050/7000] Loss: 0.4100(0.4358) Grad: 1.2198  LR: 0.000008  Elapsed: 6m 23s (remain 15m 25s) Max mem: 834 MB\n",
      "Epoch: [3/3][2100/7000] Loss: 0.3564(0.4360) Grad: 0.9247  LR: 0.000008  Elapsed: 6m 32s (remain 15m 16s) Max mem: 834 MB\n",
      "Epoch: [3/3][2150/7000] Loss: 0.3709(0.4359) Grad: 1.1508  LR: 0.000008  Elapsed: 6m 41s (remain 15m 6s) Max mem: 834 MB\n",
      "Epoch: [3/3][2200/7000] Loss: 0.4087(0.4359) Grad: 0.7140  LR: 0.000008  Elapsed: 6m 51s (remain 14m 56s) Max mem: 834 MB\n",
      "Epoch: [3/3][2250/7000] Loss: 0.4997(0.4361) Grad: 1.2052  LR: 0.000008  Elapsed: 7m 0s (remain 14m 46s) Max mem: 834 MB\n",
      "Epoch: [3/3][2300/7000] Loss: 0.4005(0.4364) Grad: 0.8867  LR: 0.000008  Elapsed: 7m 9s (remain 14m 37s) Max mem: 834 MB\n",
      "Epoch: [3/3][2350/7000] Loss: 0.3957(0.4363) Grad: 1.0864  LR: 0.000008  Elapsed: 7m 18s (remain 14m 27s) Max mem: 834 MB\n",
      "Epoch: [3/3][2400/7000] Loss: 0.4990(0.4362) Grad: 1.2963  LR: 0.000008  Elapsed: 7m 28s (remain 14m 18s) Max mem: 834 MB\n",
      "Epoch: [3/3][2450/7000] Loss: 0.3926(0.4366) Grad: 1.0667  LR: 0.000008  Elapsed: 7m 37s (remain 14m 9s) Max mem: 834 MB\n",
      "Epoch: [3/3][2500/7000] Loss: 0.4221(0.4365) Grad: 1.1408  LR: 0.000008  Elapsed: 7m 46s (remain 13m 59s) Max mem: 834 MB\n",
      "Epoch: [3/3][2550/7000] Loss: 0.4744(0.4366) Grad: 1.7496  LR: 0.000008  Elapsed: 7m 56s (remain 13m 50s) Max mem: 834 MB\n",
      "Epoch: [3/3][2600/7000] Loss: 0.4340(0.4365) Grad: 0.9266  LR: 0.000008  Elapsed: 8m 5s (remain 13m 41s) Max mem: 834 MB\n",
      "Epoch: [3/3][2650/7000] Loss: 0.3773(0.4364) Grad: 0.8706  LR: 0.000008  Elapsed: 8m 14s (remain 13m 31s) Max mem: 834 MB\n",
      "Epoch: [3/3][2700/7000] Loss: 0.5814(0.4367) Grad: 1.3375  LR: 0.000008  Elapsed: 8m 24s (remain 13m 22s) Max mem: 834 MB\n",
      "Epoch: [3/3][2750/7000] Loss: 0.3865(0.4366) Grad: 1.1058  LR: 0.000008  Elapsed: 8m 33s (remain 13m 13s) Max mem: 834 MB\n",
      "Epoch: [3/3][2800/7000] Loss: 0.4085(0.4366) Grad: 0.9551  LR: 0.000008  Elapsed: 8m 42s (remain 13m 3s) Max mem: 834 MB\n",
      "Epoch: [3/3][2850/7000] Loss: 0.4149(0.4365) Grad: 1.1144  LR: 0.000008  Elapsed: 8m 51s (remain 12m 53s) Max mem: 834 MB\n",
      "Epoch: [3/3][2900/7000] Loss: 0.5138(0.4367) Grad: 0.9874  LR: 0.000008  Elapsed: 9m 1s (remain 12m 44s) Max mem: 834 MB\n",
      "Epoch: [3/3][2950/7000] Loss: 0.4549(0.4368) Grad: 1.0305  LR: 0.000008  Elapsed: 9m 10s (remain 12m 34s) Max mem: 834 MB\n",
      "Epoch: [3/3][3000/7000] Loss: 0.3447(0.4369) Grad: 0.9773  LR: 0.000008  Elapsed: 9m 19s (remain 12m 25s) Max mem: 834 MB\n",
      "Epoch: [3/3][3050/7000] Loss: 0.3427(0.4369) Grad: 1.0563  LR: 0.000008  Elapsed: 9m 28s (remain 12m 16s) Max mem: 834 MB\n",
      "Epoch: [3/3][3100/7000] Loss: 0.3114(0.4369) Grad: 0.8580  LR: 0.000008  Elapsed: 9m 38s (remain 12m 6s) Max mem: 834 MB\n",
      "Epoch: [3/3][3150/7000] Loss: 0.4552(0.4368) Grad: 1.2079  LR: 0.000008  Elapsed: 9m 47s (remain 11m 57s) Max mem: 834 MB\n",
      "Epoch: [3/3][3200/7000] Loss: 0.4758(0.4367) Grad: 1.0085  LR: 0.000008  Elapsed: 9m 56s (remain 11m 48s) Max mem: 834 MB\n",
      "Epoch: [3/3][3250/7000] Loss: 0.3832(0.4366) Grad: 0.9060  LR: 0.000008  Elapsed: 10m 5s (remain 11m 38s) Max mem: 834 MB\n",
      "Epoch: [3/3][3300/7000] Loss: 0.4335(0.4365) Grad: 1.1462  LR: 0.000008  Elapsed: 10m 15s (remain 11m 29s) Max mem: 834 MB\n",
      "Epoch: [3/3][3350/7000] Loss: 0.4988(0.4367) Grad: 1.1942  LR: 0.000008  Elapsed: 10m 24s (remain 11m 20s) Max mem: 834 MB\n",
      "Epoch: [3/3][3400/7000] Loss: 0.2713(0.4365) Grad: 0.8357  LR: 0.000008  Elapsed: 10m 33s (remain 11m 10s) Max mem: 834 MB\n",
      "Epoch: [3/3][3450/7000] Loss: 0.4887(0.4366) Grad: 1.1129  LR: 0.000008  Elapsed: 10m 43s (remain 11m 1s) Max mem: 834 MB\n",
      "Epoch: [3/3][3500/7000] Loss: 0.4227(0.4366) Grad: 1.0622  LR: 0.000008  Elapsed: 10m 52s (remain 10m 52s) Max mem: 834 MB\n",
      "Epoch: [3/3][3550/7000] Loss: 0.4254(0.4367) Grad: 1.2508  LR: 0.000008  Elapsed: 11m 1s (remain 10m 42s) Max mem: 834 MB\n",
      "Epoch: [3/3][3600/7000] Loss: 0.4264(0.4366) Grad: 0.9968  LR: 0.000008  Elapsed: 11m 11s (remain 10m 33s) Max mem: 834 MB\n",
      "Epoch: [3/3][3650/7000] Loss: 0.5144(0.4364) Grad: 1.3241  LR: 0.000008  Elapsed: 11m 20s (remain 10m 24s) Max mem: 834 MB\n",
      "Epoch: [3/3][3700/7000] Loss: 0.4768(0.4363) Grad: 1.1761  LR: 0.000008  Elapsed: 11m 29s (remain 10m 14s) Max mem: 834 MB\n",
      "Epoch: [3/3][3750/7000] Loss: 0.5235(0.4363) Grad: 1.4771  LR: 0.000008  Elapsed: 11m 38s (remain 10m 5s) Max mem: 834 MB\n",
      "Epoch: [3/3][3800/7000] Loss: 0.3598(0.4362) Grad: 0.7872  LR: 0.000008  Elapsed: 11m 47s (remain 9m 55s) Max mem: 834 MB\n",
      "Epoch: [3/3][3850/7000] Loss: 0.3336(0.4362) Grad: 0.8538  LR: 0.000008  Elapsed: 11m 57s (remain 9m 46s) Max mem: 834 MB\n",
      "Epoch: [3/3][3900/7000] Loss: 0.3218(0.4360) Grad: 0.7953  LR: 0.000008  Elapsed: 12m 6s (remain 9m 37s) Max mem: 834 MB\n",
      "Epoch: [3/3][3950/7000] Loss: 0.4030(0.4359) Grad: 0.9281  LR: 0.000008  Elapsed: 12m 16s (remain 9m 28s) Max mem: 834 MB\n",
      "Epoch: [3/3][4000/7000] Loss: 0.3492(0.4359) Grad: 1.3991  LR: 0.000008  Elapsed: 12m 25s (remain 9m 18s) Max mem: 834 MB\n",
      "Epoch: [3/3][4050/7000] Loss: 0.4053(0.4356) Grad: 0.8411  LR: 0.000008  Elapsed: 12m 34s (remain 9m 9s) Max mem: 834 MB\n",
      "Epoch: [3/3][4100/7000] Loss: 0.5148(0.4356) Grad: 1.2470  LR: 0.000008  Elapsed: 12m 43s (remain 9m 0s) Max mem: 834 MB\n",
      "Epoch: [3/3][4150/7000] Loss: 0.3606(0.4355) Grad: 1.2328  LR: 0.000008  Elapsed: 12m 53s (remain 8m 50s) Max mem: 834 MB\n",
      "Epoch: [3/3][4200/7000] Loss: 0.4887(0.4354) Grad: 1.1494  LR: 0.000008  Elapsed: 13m 2s (remain 8m 41s) Max mem: 834 MB\n",
      "Epoch: [3/3][4250/7000] Loss: 0.4919(0.4357) Grad: 1.3779  LR: 0.000008  Elapsed: 13m 11s (remain 8m 31s) Max mem: 834 MB\n",
      "Epoch: [3/3][4300/7000] Loss: 0.4497(0.4356) Grad: 1.4298  LR: 0.000008  Elapsed: 13m 20s (remain 8m 22s) Max mem: 834 MB\n",
      "Epoch: [3/3][4350/7000] Loss: 0.4331(0.4357) Grad: 1.0126  LR: 0.000008  Elapsed: 13m 30s (remain 8m 13s) Max mem: 834 MB\n",
      "Epoch: [3/3][4400/7000] Loss: 0.4831(0.4358) Grad: 1.0402  LR: 0.000008  Elapsed: 13m 39s (remain 8m 3s) Max mem: 834 MB\n",
      "Epoch: [3/3][4450/7000] Loss: 0.4777(0.4357) Grad: 1.1673  LR: 0.000008  Elapsed: 13m 48s (remain 7m 54s) Max mem: 834 MB\n",
      "Epoch: [3/3][4500/7000] Loss: 0.4880(0.4357) Grad: 1.0632  LR: 0.000008  Elapsed: 13m 57s (remain 7m 45s) Max mem: 834 MB\n",
      "Epoch: [3/3][4550/7000] Loss: 0.4498(0.4357) Grad: 1.1354  LR: 0.000008  Elapsed: 14m 6s (remain 7m 35s) Max mem: 834 MB\n",
      "Epoch: [3/3][4600/7000] Loss: 0.4576(0.4359) Grad: 0.8874  LR: 0.000008  Elapsed: 14m 16s (remain 7m 26s) Max mem: 834 MB\n",
      "Epoch: [3/3][4650/7000] Loss: 0.4471(0.4360) Grad: 1.0956  LR: 0.000008  Elapsed: 14m 25s (remain 7m 17s) Max mem: 834 MB\n",
      "Epoch: [3/3][4700/7000] Loss: 0.4155(0.4360) Grad: 0.8628  LR: 0.000008  Elapsed: 14m 34s (remain 7m 7s) Max mem: 834 MB\n",
      "Epoch: [3/3][4750/7000] Loss: 0.3320(0.4360) Grad: 1.0891  LR: 0.000008  Elapsed: 14m 44s (remain 6m 58s) Max mem: 834 MB\n",
      "Epoch: [3/3][4800/7000] Loss: 0.4180(0.4361) Grad: 0.9739  LR: 0.000008  Elapsed: 14m 53s (remain 6m 49s) Max mem: 834 MB\n",
      "Epoch: [3/3][4850/7000] Loss: 0.4279(0.4360) Grad: 0.7982  LR: 0.000008  Elapsed: 15m 2s (remain 6m 39s) Max mem: 834 MB\n",
      "Epoch: [3/3][4900/7000] Loss: 0.3791(0.4361) Grad: 1.1280  LR: 0.000008  Elapsed: 15m 11s (remain 6m 30s) Max mem: 834 MB\n",
      "Epoch: [3/3][4950/7000] Loss: 0.3438(0.4360) Grad: 1.0706  LR: 0.000008  Elapsed: 15m 21s (remain 6m 21s) Max mem: 834 MB\n",
      "Epoch: [3/3][5000/7000] Loss: 0.4624(0.4360) Grad: 1.2612  LR: 0.000008  Elapsed: 15m 30s (remain 6m 11s) Max mem: 834 MB\n",
      "Epoch: [3/3][5050/7000] Loss: 0.4493(0.4358) Grad: 1.4462  LR: 0.000008  Elapsed: 15m 39s (remain 6m 2s) Max mem: 834 MB\n",
      "Epoch: [3/3][5100/7000] Loss: 0.4720(0.4358) Grad: 0.9925  LR: 0.000008  Elapsed: 15m 49s (remain 5m 53s) Max mem: 834 MB\n",
      "Epoch: [3/3][5150/7000] Loss: 0.2930(0.4357) Grad: 1.0210  LR: 0.000008  Elapsed: 15m 58s (remain 5m 44s) Max mem: 834 MB\n",
      "Epoch: [3/3][5200/7000] Loss: 0.4930(0.4357) Grad: 1.1224  LR: 0.000008  Elapsed: 16m 7s (remain 5m 34s) Max mem: 834 MB\n",
      "Epoch: [3/3][5250/7000] Loss: 0.2348(0.4355) Grad: 0.7814  LR: 0.000008  Elapsed: 16m 16s (remain 5m 25s) Max mem: 834 MB\n",
      "Epoch: [3/3][5300/7000] Loss: 0.4025(0.4356) Grad: 0.8569  LR: 0.000008  Elapsed: 16m 26s (remain 5m 16s) Max mem: 834 MB\n",
      "Epoch: [3/3][5350/7000] Loss: 0.5388(0.4355) Grad: 1.2442  LR: 0.000008  Elapsed: 16m 35s (remain 5m 6s) Max mem: 834 MB\n",
      "Epoch: [3/3][5400/7000] Loss: 0.3508(0.4354) Grad: 0.9088  LR: 0.000008  Elapsed: 16m 44s (remain 4m 57s) Max mem: 834 MB\n",
      "Epoch: [3/3][5450/7000] Loss: 0.5940(0.4353) Grad: 1.5811  LR: 0.000008  Elapsed: 16m 53s (remain 4m 48s) Max mem: 834 MB\n",
      "Epoch: [3/3][5500/7000] Loss: 0.4712(0.4352) Grad: 1.0686  LR: 0.000008  Elapsed: 17m 2s (remain 4m 38s) Max mem: 834 MB\n",
      "Epoch: [3/3][5550/7000] Loss: 0.3538(0.4353) Grad: 0.9467  LR: 0.000008  Elapsed: 17m 12s (remain 4m 29s) Max mem: 834 MB\n",
      "Epoch: [3/3][5600/7000] Loss: 0.4873(0.4353) Grad: 1.4994  LR: 0.000008  Elapsed: 17m 21s (remain 4m 20s) Max mem: 834 MB\n",
      "Epoch: [3/3][5650/7000] Loss: 0.5469(0.4353) Grad: 1.5386  LR: 0.000008  Elapsed: 17m 30s (remain 4m 10s) Max mem: 834 MB\n",
      "Epoch: [3/3][5700/7000] Loss: 0.3535(0.4354) Grad: 1.2644  LR: 0.000008  Elapsed: 17m 39s (remain 4m 1s) Max mem: 834 MB\n",
      "Epoch: [3/3][5750/7000] Loss: 0.4409(0.4354) Grad: 1.1864  LR: 0.000008  Elapsed: 17m 49s (remain 3m 52s) Max mem: 834 MB\n",
      "Epoch: [3/3][5800/7000] Loss: 0.4694(0.4354) Grad: 1.5609  LR: 0.000008  Elapsed: 17m 58s (remain 3m 42s) Max mem: 834 MB\n",
      "Epoch: [3/3][5850/7000] Loss: 0.3717(0.4354) Grad: 1.1240  LR: 0.000008  Elapsed: 18m 7s (remain 3m 33s) Max mem: 834 MB\n",
      "Epoch: [3/3][5900/7000] Loss: 0.4622(0.4352) Grad: 1.4290  LR: 0.000008  Elapsed: 18m 16s (remain 3m 24s) Max mem: 834 MB\n",
      "Epoch: [3/3][5950/7000] Loss: 0.4928(0.4352) Grad: 0.9101  LR: 0.000008  Elapsed: 18m 25s (remain 3m 14s) Max mem: 834 MB\n",
      "Epoch: [3/3][6000/7000] Loss: 0.3410(0.4352) Grad: 0.8424  LR: 0.000008  Elapsed: 18m 35s (remain 3m 5s) Max mem: 834 MB\n",
      "Epoch: [3/3][6050/7000] Loss: 0.4899(0.4353) Grad: 1.0315  LR: 0.000008  Elapsed: 18m 44s (remain 2m 56s) Max mem: 834 MB\n",
      "Epoch: [3/3][6100/7000] Loss: 0.4848(0.4354) Grad: 1.1771  LR: 0.000008  Elapsed: 18m 53s (remain 2m 47s) Max mem: 834 MB\n",
      "Epoch: [3/3][6150/7000] Loss: 0.4775(0.4353) Grad: 1.3058  LR: 0.000008  Elapsed: 19m 3s (remain 2m 37s) Max mem: 834 MB\n",
      "Epoch: [3/3][6200/7000] Loss: 0.3551(0.4353) Grad: 0.9645  LR: 0.000008  Elapsed: 19m 12s (remain 2m 28s) Max mem: 834 MB\n",
      "Epoch: [3/3][6250/7000] Loss: 0.3636(0.4353) Grad: 0.9650  LR: 0.000008  Elapsed: 19m 21s (remain 2m 19s) Max mem: 834 MB\n",
      "Epoch: [3/3][6300/7000] Loss: 0.3685(0.4354) Grad: 1.2406  LR: 0.000008  Elapsed: 19m 31s (remain 2m 9s) Max mem: 834 MB\n",
      "Epoch: [3/3][6350/7000] Loss: 0.4723(0.4354) Grad: 1.1898  LR: 0.000008  Elapsed: 19m 40s (remain 2m 0s) Max mem: 834 MB\n",
      "Epoch: [3/3][6400/7000] Loss: 0.4279(0.4353) Grad: 1.0671  LR: 0.000008  Elapsed: 19m 49s (remain 1m 51s) Max mem: 834 MB\n",
      "Epoch: [3/3][6450/7000] Loss: 0.4049(0.4353) Grad: 0.9090  LR: 0.000008  Elapsed: 19m 59s (remain 1m 42s) Max mem: 834 MB\n",
      "Epoch: [3/3][6500/7000] Loss: 0.4328(0.4354) Grad: 1.2413  LR: 0.000008  Elapsed: 20m 8s (remain 1m 32s) Max mem: 834 MB\n",
      "Epoch: [3/3][6550/7000] Loss: 0.4123(0.4353) Grad: 1.1901  LR: 0.000008  Elapsed: 20m 17s (remain 1m 23s) Max mem: 834 MB\n",
      "Epoch: [3/3][6600/7000] Loss: 0.6287(0.4354) Grad: 2.0883  LR: 0.000008  Elapsed: 20m 26s (remain 1m 14s) Max mem: 834 MB\n",
      "Epoch: [3/3][6650/7000] Loss: 0.3838(0.4354) Grad: 1.4405  LR: 0.000008  Elapsed: 20m 36s (remain 1m 4s) Max mem: 834 MB\n",
      "Epoch: [3/3][6700/7000] Loss: 0.4146(0.4354) Grad: 1.0732  LR: 0.000008  Elapsed: 20m 45s (remain 0m 55s) Max mem: 834 MB\n",
      "Epoch: [3/3][6750/7000] Loss: 0.3467(0.4354) Grad: 1.0921  LR: 0.000008  Elapsed: 20m 54s (remain 0m 46s) Max mem: 834 MB\n",
      "Epoch: [3/3][6800/7000] Loss: 0.4329(0.4353) Grad: 1.1803  LR: 0.000008  Elapsed: 21m 4s (remain 0m 36s) Max mem: 834 MB\n",
      "Epoch: [3/3][6850/7000] Loss: 0.3668(0.4353) Grad: 1.1413  LR: 0.000008  Elapsed: 21m 13s (remain 0m 27s) Max mem: 834 MB\n",
      "Epoch: [3/3][6900/7000] Loss: 0.4539(0.4353) Grad: 1.1260  LR: 0.000008  Elapsed: 21m 22s (remain 0m 18s) Max mem: 834 MB\n",
      "Epoch: [3/3][6950/7000] Loss: 0.4781(0.4354) Grad: 1.2600  LR: 0.000008  Elapsed: 21m 32s (remain 0m 9s) Max mem: 834 MB\n",
      "EVAL: [0/875] Data 1.670 (1.670) Elapsed 0m 1s (remain 25m 17s) Loss: 0.4597(0.4597) \n",
      "EVAL: [50/875] Data 0.002 (0.125) Elapsed 0m 9s (remain 2m 33s) Loss: 0.4694(0.4411) \n",
      "EVAL: [100/875] Data 1.237 (0.122) Elapsed 0m 18s (remain 2m 21s) Loss: 0.5233(0.4381) \n",
      "EVAL: [150/875] Data 0.002 (0.124) Elapsed 0m 28s (remain 2m 14s) Loss: 0.4154(0.4359) \n",
      "EVAL: [200/875] Data 1.430 (0.131) Elapsed 0m 38s (remain 2m 10s) Loss: 0.4267(0.4379) \n",
      "EVAL: [250/875] Data 0.002 (0.131) Elapsed 0m 48s (remain 2m 0s) Loss: 0.3818(0.4387) \n",
      "EVAL: [300/875] Data 0.002 (0.128) Elapsed 0m 57s (remain 1m 48s) Loss: 0.4362(0.4386) \n",
      "EVAL: [350/875] Data 0.002 (0.130) Elapsed 1m 7s (remain 1m 40s) Loss: 0.3961(0.4384) \n",
      "EVAL: [400/875] Data 0.002 (0.128) Elapsed 1m 16s (remain 1m 30s) Loss: 0.4042(0.4381) \n",
      "EVAL: [450/875] Data 0.002 (0.129) Elapsed 1m 26s (remain 1m 21s) Loss: 0.3820(0.4371) \n",
      "EVAL: [500/875] Data 0.002 (0.127) Elapsed 1m 34s (remain 1m 10s) Loss: 0.4542(0.4378) \n",
      "EVAL: [550/875] Data 0.002 (0.130) Elapsed 1m 45s (remain 1m 2s) Loss: 0.4286(0.4372) \n",
      "EVAL: [600/875] Data 0.002 (0.128) Elapsed 1m 54s (remain 0m 52s) Loss: 0.4726(0.4360) \n",
      "EVAL: [650/875] Data 0.002 (0.129) Elapsed 2m 4s (remain 0m 42s) Loss: 0.4902(0.4363) \n",
      "EVAL: [700/875] Data 0.002 (0.129) Elapsed 2m 13s (remain 0m 33s) Loss: 0.4313(0.4363) \n",
      "EVAL: [750/875] Data 0.002 (0.129) Elapsed 2m 23s (remain 0m 23s) Loss: 0.4115(0.4365) \n",
      "EVAL: [800/875] Data 0.002 (0.129) Elapsed 2m 32s (remain 0m 14s) Loss: 0.4818(0.4360) \n",
      "EVAL: [850/875] Data 1.273 (0.129) Elapsed 2m 42s (remain 0m 4s) Loss: 0.4332(0.4366) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3 - avg_train_loss: 0.4354  avg_val_loss: 0.4365  time: 1466s\n",
      "Epoch 3 - Score: 0.8565\n",
      "Epoch 3 - Save Best Score: 0.8565 Model\n",
      "Epoch 3 - Save Best Loss: 0.4365 Model\n",
      "========== fold: 0 result ==========\n",
      "Score: 0.8565\n",
      "========== CV ==========\n",
      "Score: 0.8565\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "first-austin",
   "metadata": {
    "papermill": {
     "duration": 0.180664,
     "end_time": "2023-11-21T11:17:08.446715",
     "exception": false,
     "start_time": "2023-11-21T11:17:08.266051",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "databundleVersionId": 2399555,
     "sourceId": 23249,
     "sourceType": "competition"
    },
    {
     "datasetId": 1515231,
     "sourceId": 2502247,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 1516245,
     "sourceId": 2503871,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 1516251,
     "sourceId": 2503877,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 1516252,
     "sourceId": 2503879,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 686792,
     "sourceId": 2660070,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30097,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 4847.990321,
   "end_time": "2023-11-21T11:17:11.704499",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2023-11-21T09:56:23.714178",
   "version": "2.3.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
